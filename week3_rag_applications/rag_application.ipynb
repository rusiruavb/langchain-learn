{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Advanced LangChain & RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "## üìö Session Overview\n",
    "\n",
    "**Duration:** 2 hours  \n",
    "**Week:** 3  \n",
    "**Instructor-Led Session**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. Understand what RAG is and why it's important\n",
    "2. Work with embeddings and vector stores\n",
    "3. Set up and use PGVector for semantic search\n",
    "4. Process and chunk documents effectively\n",
    "5. Build complete RAG applications with LangChain\n",
    "6. Implement conversational RAG with memory\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- ‚úÖ Completed Week 1 & 2\n",
    "- ‚úÖ Understanding of LangChain chains\n",
    "- ‚úÖ PostgreSQL with PGVector extension installed\n",
    "- ‚úÖ Docker running (for PGVector)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "\n",
    "- Setup & Introduction: 10 minutes\n",
    "- Section 1 (RAG Introduction): 20 minutes\n",
    "- Section 2 (Embeddings & Vectors): 25 minutes\n",
    "- Section 3 (Document Processing): 20 minutes\n",
    "- Section 4 (Building RAG Apps): 35 minutes\n",
    "- Section 5 (Conversational RAG): 15 minutes\n",
    "- Wrap-up & Q&A: 5 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import PGVector\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Standard imports\n",
    "from typing import List\n",
    "import textwrap\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Introduction to RAG (20 minutes)\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** combines the power of retrieval systems with generative AI.\n",
    "\n",
    "### The Problem RAG Solves\n",
    "\n",
    "**Without RAG:**\n",
    "- ‚ùå LLMs have a knowledge cutoff date\n",
    "- ‚ùå Can't access private/proprietary data\n",
    "- ‚ùå Limited by training data\n",
    "- ‚ùå May hallucinate facts\n",
    "\n",
    "**With RAG:**\n",
    "- ‚úÖ Access to current information\n",
    "- ‚úÖ Use your own documents/data\n",
    "- ‚úÖ Grounded, factual responses\n",
    "- ‚úÖ Cite sources\n",
    "\n",
    "---\n",
    "\n",
    "## How RAG Works\n",
    "\n",
    "```\n",
    "User Question\n",
    "     ‚Üì\n",
    "1. Convert to embedding (vector)\n",
    "     ‚Üì\n",
    "2. Search vector database for similar documents\n",
    "     ‚Üì\n",
    "3. Retrieve top K most relevant documents\n",
    "     ‚Üì\n",
    "4. Combine question + retrieved docs in prompt\n",
    "     ‚Üì\n",
    "5. LLM generates answer based on context\n",
    "     ‚Üì\n",
    "Answer with sources\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. **Embeddings**\n",
    "- Convert text to numerical vectors (arrays of numbers)\n",
    "- Similar meaning = similar vectors\n",
    "- Example: OpenAI's `text-embedding-3-small` creates 1536-dimensional vectors\n",
    "\n",
    "### 2. **Vector Store**\n",
    "- Database optimized for vector similarity search\n",
    "- Examples: PGVector, Pinecone, Chroma, FAISS\n",
    "- We'll use **PGVector** (PostgreSQL extension)\n",
    "\n",
    "### 3. **Document Loaders**\n",
    "- Load documents from various sources\n",
    "- PDF, TXT, CSV, web pages, etc.\n",
    "\n",
    "### 4. **Text Splitters**\n",
    "- Break documents into smaller chunks\n",
    "- Preserve semantic meaning\n",
    "- Manage token limits\n",
    "\n",
    "### 5. **Retriever**\n",
    "- Searches vector store\n",
    "- Returns most relevant chunks\n",
    "\n",
    "---\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **Customer Support:** Answer questions from documentation\n",
    "- **Research Assistant:** Search through papers/articles\n",
    "- **Internal Knowledge Base:** Company policies, procedures\n",
    "- **Legal/Compliance:** Search contracts and regulations\n",
    "- **Code Assistant:** Search codebase and documentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Embeddings & Vector Stores (25 minutes)\n",
    "\n",
    "## Understanding Embeddings\n",
    "\n",
    "Embeddings are **numerical representations** of text that capture semantic meaning.\n",
    "\n",
    "### How Embeddings Work\n",
    "\n",
    "```python\n",
    "Text: \"The cat sits on the mat\"\n",
    "     ‚Üì (Embedding Model)\n",
    "Vector: [0.23, -0.45, 0.67, ..., 0.12]  # 1536 numbers\n",
    "```\n",
    "\n",
    "**Similar texts ‚Üí Similar vectors:**\n",
    "```\n",
    "\"cat on mat\"     ‚Üí [0.23, -0.45, 0.67, ...]\n",
    "\"feline on rug\"  ‚Üí [0.25, -0.43, 0.69, ...]  # Very similar!\n",
    "\"car in garage\"  ‚Üí [0.89, 0.12, -0.34, ...]  # Very different!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Embedding Info:\n",
      "Text: What is artificial intelligence?\n",
      "Vector dimension: 1536\n",
      "First 10 values: [0.006159287411719561, -0.014519386924803257, -0.03583042323589325, 0.005716137588024139, 0.021948374807834625, -0.037045348435640335, -0.03624867647886276, 0.01474843081086874, -0.01701895333826542, 0.023860393092036247]\n",
      "Vector type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Create embedding for a single text\n",
    "text = \"What is artificial intelligence?\"\n",
    "embedding_vector = embeddings.embed_query(text)\n",
    "\n",
    "print(\"üìä Embedding Info:\")\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Vector dimension: {len(embedding_vector)}\")\n",
    "print(f\"First 10 values: {embedding_vector[:10]}\")\n",
    "print(f\"Vector type: {type(embedding_vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Embedded 3 documents\n",
      "Each embedding has 1536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Embed multiple documents\n",
    "documents = [\n",
    "    \"Python is a programming language.\",\n",
    "    \"Machine learning is a subset of AI.\",\n",
    "    \"Neural networks are inspired by the human brain.\"\n",
    "]\n",
    "\n",
    "doc_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "print(f\"üìö Embedded {len(doc_embeddings)} documents\")\n",
    "print(f\"Each embedding has {len(doc_embeddings[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Between Embeddings\n",
    "\n",
    "We can measure similarity using **cosine similarity** or **dot product**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Similarity Scores:\n",
      "Text 1 vs Text 2 (similar): 0.6870\n",
      "Text 1 vs Text 3 (different): 0.3084\n",
      "Text 2 vs Text 3 (different): 0.3101\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Compare similarities\n",
    "texts = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python is my favorite coding language\",\n",
    "    \"I enjoy eating pizza\"\n",
    "]\n",
    "\n",
    "vecs = [embeddings.embed_query(t) for t in texts]\n",
    "\n",
    "print(\"üîç Similarity Scores:\")\n",
    "print(f\"Text 1 vs Text 2 (similar): {cosine_similarity(vecs[0], vecs[1]):.4f}\")\n",
    "print(f\"Text 1 vs Text 3 (different): {cosine_similarity(vecs[0], vecs[2]):.4f}\")\n",
    "print(f\"Text 2 vs Text 3 (different): {cosine_similarity(vecs[1], vecs[2]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2: Setting Up PGVector\n",
    "\n",
    "**PGVector** is a PostgreSQL extension for storing and searching vector embeddings.\n",
    "\n",
    "### Why PGVector?\n",
    "\n",
    "- ‚úÖ Built on reliable PostgreSQL\n",
    "- ‚úÖ ACID compliance (transactions)\n",
    "- ‚úÖ Can store vectors alongside regular data\n",
    "- ‚úÖ Fast similarity search\n",
    "- ‚úÖ Open source and free\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PGVector Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connection string configured\n",
      "üì¶ Collection name: week3_demo\n"
     ]
    }
   ],
   "source": [
    "# Database connection string\n",
    "# Format: postgresql://username:password@host:port/database\n",
    "CONNECTION_STRING = os.getenv(\n",
    "    \"DATABASE_URL\",\n",
    "    \"postgresql://postgres:postgres@localhost:5432/ai_agent_course\"\n",
    ")\n",
    "\n",
    "# Collection name (like a table for this use case)\n",
    "COLLECTION_NAME = \"week3_demo\"\n",
    "\n",
    "print(f\"‚úÖ Connection string configured\")\n",
    "print(f\"üì¶ Collection name: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Documents in PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored 4 documents in PGVector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rusirubandara/Documents/code/wireapps/intern/ai-program/venv/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:490: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create sample documents\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"Python is a high-level programming language known for its simplicity.\",\n",
    "        metadata={\"source\": \"python_intro.txt\", \"category\": \"programming\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning enables computers to learn from data without explicit programming.\",\n",
    "        metadata={\"source\": \"ml_basics.txt\", \"category\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain is a framework for developing applications powered by language models.\",\n",
    "        metadata={\"source\": \"langchain_docs.txt\", \"category\": \"framework\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases store embeddings and enable semantic search capabilities.\",\n",
    "        metadata={\"source\": \"vector_db.txt\", \"category\": \"database\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create vector store and add documents\n",
    "vectorstore = PGVector.from_documents(\n",
    "    documents=sample_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Stored {len(sample_docs)} documents in PGVector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Vector Store (Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What is a framework for building AI applications?\n",
      "\n",
      "üìä Top 2 Results:\n",
      "\n",
      "1. LangChain is a framework for developing applications powered by language models.\n",
      "   Source: langchain_docs.txt\n",
      "   Category: framework\n",
      "\n",
      "2. LangChain is a framework for developing applications powered by language models.\n",
      "   Source: langchain_docs.txt\n",
      "   Category: framework\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform similarity search\n",
    "query = \"What is a framework for building AI applications?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"üîç Query: {query}\")\n",
    "print(f\"\\nüìä Top {len(results)} Results:\\n\")\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "    print(f\"   Source: {doc.metadata['source']}\")\n",
    "    print(f\"   Category: {doc.metadata['category']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What is a framework for building AI applications?\n",
      "\n",
      "üìä Results with Similarity Scores:\n",
      "\n",
      "Score: 0.5938\n",
      "Content: LangChain is a framework for developing applications powered by language models.\n",
      "Source: langchain_docs.txt\n",
      "------------------------------------------------------------\n",
      "Score: 0.5938\n",
      "Content: LangChain is a framework for developing applications powered by language models.\n",
      "Source: langchain_docs.txt\n",
      "------------------------------------------------------------\n",
      "Score: 0.5939\n",
      "Content: LangChain is a framework for developing applications powered by language models.\n",
      "Source: langchain_docs.txt\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get similarity scores\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"üîç Query: {query}\")\n",
    "print(f\"\\nüìä Results with Similarity Scores:\\n\")\n",
    "\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Filtered Search (category='programming'):\n",
      "- Python is a high-level programming language known for its simplicity.\n",
      "  Category: programming\n",
      "\n",
      "- Python is a high-level programming language known for its simplicity.\n",
      "  Category: programming\n",
      "\n",
      "- Python is a high-level programming language known for its simplicity.\n",
      "  Category: programming\n",
      "\n",
      "- Python is a high-level programming language known for its simplicity.\n",
      "  Category: programming\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search with metadata filter\n",
    "filtered_results = vectorstore.similarity_search(\n",
    "    query=\"programming\",\n",
    "    k=5,\n",
    "    filter={\"category\": \"programming\"}\n",
    ")\n",
    "\n",
    "print(\"üîç Filtered Search (category='programming'):\")\n",
    "for doc in filtered_results:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "    print(f\"  Category: {doc.metadata['category']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Document Processing (20 minutes)\n",
    "\n",
    "## Why Document Processing Matters\n",
    "\n",
    "**Challenges:**\n",
    "- Documents are often too long for LLM context windows\n",
    "- Need to break into meaningful chunks\n",
    "- Must preserve context and relationships\n",
    "- Different file formats require different handling\n",
    "\n",
    "**Solution:**\n",
    "- **Document Loaders:** Extract text from various formats\n",
    "- **Text Splitters:** Intelligently chunk documents\n",
    "- **Metadata:** Track source and context\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample document created\n"
     ]
    }
   ],
   "source": [
    "# Create a sample text file\n",
    "sample_text = \"\"\"\n",
    "Introduction to Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence by machines.\n",
    "AI systems can perform tasks that typically require human intelligence, such as\n",
    "visual perception, speech recognition, decision-making, and language translation.\n",
    "\n",
    "Types of AI:\n",
    "1. Narrow AI: Designed for specific tasks (e.g., image recognition)\n",
    "2. General AI: Theoretical AI with human-like intelligence\n",
    "3. Super AI: Hypothetical AI that surpasses human intelligence\n",
    "\n",
    "Machine Learning is a subset of AI that enables systems to learn from data.\n",
    "Deep Learning is a subset of Machine Learning using neural networks.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"sample_ai_doc.txt\", \"w\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(\"‚úÖ Sample document created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 1 document(s)\n",
      "\n",
      "Document content preview:\n",
      "\n",
      "Introduction to Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) is the simulation of human intelligence by machines.\n",
      "AI systems can perform tasks that typically require human intelligence, such...\n",
      "\n",
      "Metadata: {'source': 'sample_ai_doc.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Load text document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"sample_ai_doc.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"üìÑ Loaded {len(documents)} document(s)\")\n",
    "print(f\"\\nDocument content preview:\")\n",
    "print(documents[0].page_content[:200] + \"...\")\n",
    "print(f\"\\nMetadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2: Text Splitting Strategies\n",
    "\n",
    "### Why Split Text?\n",
    "\n",
    "1. **Token Limits:** LLMs have maximum context length\n",
    "2. **Relevance:** Smaller chunks = more precise retrieval\n",
    "3. **Performance:** Faster search with smaller chunks\n",
    "4. **Cost:** Only send relevant context to LLM\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "- **chunk_size:** Target size of each chunk (in characters)\n",
    "- **chunk_overlap:** Overlap between chunks to preserve context\n",
    "- **separators:** How to split (by sentence, paragraph, etc.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter\n",
    "\n",
    "Tries to split on different separators in order of preference:\n",
    "1. Double newlines (paragraphs)\n",
    "2. Single newlines (lines)\n",
    "3. Spaces (words)\n",
    "4. Characters (as last resort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original document split into 9 chunks\n",
      "\n",
      "Chunk 1 (length: 39):\n",
      "Introduction to Artificial Intelligence\n",
      "------------------------------------------------------------\n",
      "Chunk 2 (length: 81):\n",
      "Artificial Intelligence (AI) is the simulation of human intelligence by machines.\n",
      "------------------------------------------------------------\n",
      "Chunk 3 (length: 79):\n",
      "AI systems can perform tasks that typically require human intelligence, such as\n",
      "------------------------------------------------------------\n",
      "Chunk 4 (length: 81):\n",
      "visual perception, speech recognition, decision-making, and language translation.\n",
      "------------------------------------------------------------\n",
      "Chunk 5 (length: 80):\n",
      "Types of AI:\n",
      "1. Narrow AI: Designed for specific tasks (e.g., image recognition)\n",
      "------------------------------------------------------------\n",
      "Chunk 6 (length: 58):\n",
      "2. General AI: Theoretical AI with human-like intelligence\n",
      "------------------------------------------------------------\n",
      "Chunk 7 (length: 62):\n",
      "3. Super AI: Hypothetical AI that surpasses human intelligence\n",
      "------------------------------------------------------------\n",
      "Chunk 8 (length: 75):\n",
      "Machine Learning is a subset of AI that enables systems to learn from data.\n",
      "------------------------------------------------------------\n",
      "Chunk 9 (length: 68):\n",
      "Deep Learning is a subset of Machine Learning using neural networks.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create text splitter\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,      # Target chunk size\n",
    "    chunk_overlap=80,    # Overlap between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "print(f\"üìÑ Original document split into {len(chunks)} chunks\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} (length: {len(chunk.page_content)}):\")\n",
    "    print(chunk.page_content)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Chunk Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size 100: 9 chunks created\n",
      "Chunk size 300: 3 chunks created\n",
      "Chunk size 500: 2 chunks created\n"
     ]
    }
   ],
   "source": [
    "# Compare different chunk sizes\n",
    "chunk_sizes = [100, 300, 500]\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"Chunk size {size}: {len(chunks)} chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Custom Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced metadata for first chunk:\n",
      "{'source': 'sample_ai_doc.txt', 'chunk_id': 0, 'chunk_size': 488, 'document_name': 'AI Introduction'}\n"
     ]
    }
   ],
   "source": [
    "# Add custom metadata to chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata[\"chunk_id\"] = i\n",
    "    chunk.metadata[\"chunk_size\"] = len(chunk.page_content)\n",
    "    chunk.metadata[\"document_name\"] = \"AI Introduction\"\n",
    "\n",
    "print(\"Enhanced metadata for first chunk:\")\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Building RAG Applications (35 minutes)\n",
    "\n",
    "Now let's put it all together to build a complete RAG system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'sample_ai_doc.txt'}, page_content='Introduction to Artificial Intelligence\\n\\nArtificial Intelligence (AI) is the simulation of human intelligence by machines.\\nAI systems can perform tasks that typically require human intelligence, such as\\nvisual perception, speech recognition, decision-making, and language translation.'), Document(metadata={'source': 'sample_ai_doc.txt'}, page_content='Types of AI:\\n1. Narrow AI: Designed for specific tasks (e.g., image recognition)\\n2. General AI: Theoretical AI with human-like intelligence\\n3. Super AI: Hypothetical AI that surpasses human intelligence'), Document(metadata={'source': 'sample_ai_doc.txt'}, page_content='Machine Learning is a subset of AI that enables systems to learn from data.\\nDeep Learning is a subset of Machine Learning using neural networks.')]\n",
      "üìÑ Loaded and split into 3 chunks\n",
      "‚úÖ Vector store created\n",
      "‚úÖ Retriever configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rusirubandara/Documents/code/wireapps/intern/ai-program/venv/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:490: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and split documents\n",
    "loader = TextLoader(\"sample_ai_doc.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(splits)\n",
    "\n",
    "print(f\"üìÑ Loaded and split into {len(splits)} chunks\")\n",
    "\n",
    "# Step 2: Create vector store\n",
    "vectorstore = PGVector.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_demo\",\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store created\")\n",
    "\n",
    "# Step 3: Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 chunks\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What are the types of AI?\n",
      "\n",
      "üìö Retrieved 3 relevant chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Types of AI: 1. Narrow AI: Designed for specific tasks (e.g., image recognition)\n",
      "2. General AI: Theoretical AI with human-like intelligence 3. Super AI:\n",
      "Hypothetical AI that surpasses human intelligence\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2:\n",
      "Types of AI: 1. Narrow AI: Designed for specific tasks (e.g., image recognition)\n",
      "2. General AI: Theoretical AI with human-like intelligence 3. Super AI:\n",
      "Hypothetical AI that surpasses human intelligence\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3:\n",
      "Types of AI: 1. Narrow AI: Designed for specific tasks (e.g., image recognition)\n",
      "2. General AI: Theoretical AI with human-like intelligence 3. Super AI:\n",
      "Hypothetical AI that surpasses human intelligence\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the retriever\n",
    "query = \"What are the types of AI?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"üîç Query: {query}\")\n",
    "print(f\"\\nüìö Retrieved {len(retrieved_docs)} relevant chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(textwrap.fill(doc.page_content, width=80))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3: Build RAG Chain with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain created\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and concise answer based on the context above. \n",
    "If the answer cannot be found in the context, say \"I don't have enough information to answer that.\"\n",
    "\"\"\")\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4: Query the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: What is Artificial Intelligence?\n",
      "üí° Answer: Artificial Intelligence is the simulation of human intelligence by machines.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Question: What are the three types of AI mentioned?\n",
      "üí° Answer: The three types of AI mentioned are Narrow AI, General AI, and Super AI.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Question: How is Machine Learning related to AI?\n",
      "üí° Answer: Machine Learning is a subset of AI.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Question: What is quantum computing?\n",
      "üí° Answer: I don't have enough information to answer that.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ask questions\n",
    "questions = [\n",
    "    \"What is Artificial Intelligence?\",\n",
    "    \"What are the three types of AI mentioned?\",\n",
    "    \"How is Machine Learning related to AI?\",\n",
    "    \"What is quantum computing?\"  # Not in the context\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"üí° Answer: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5: RAG with Source Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What is Deep Learning?\n",
      "\n",
      "üí° Answer: Deep Learning is a subset of Machine Learning using neural networks.\n",
      "\n",
      "üìö Sources:\n",
      "\n",
      "1. sample_ai_doc.txt\n",
      "      Machine Learning is a subset of AI that enables systems to learn\n",
      "   from data. Deep Learning is a subset of Machine Learning using\n",
      "   neural networks.\n",
      "\n",
      "2. sample_ai_doc.txt\n",
      "      Machine Learning is a subset of AI that enables systems to learn\n",
      "   from data. Deep Learning is a subset of Machine Learning using\n",
      "   neural networks.\n",
      "\n",
      "3. sample_ai_doc.txt\n",
      "      Machine Learning is a subset of AI that enables systems to learn\n",
      "   from data. Deep Learning is a subset of Machine Learning using\n",
      "   neural networks.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RAG chain that returns sources\n",
    "def rag_with_sources(question: str):\n",
    "    \"\"\"RAG that returns answer with source documents.\"\"\"\n",
    "    \n",
    "    # Retrieve documents\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # Format context\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # Get answer\n",
    "    answer: str = rag_chain.invoke(question)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": docs\n",
    "    }\n",
    "\n",
    "# Test with sources\n",
    "result = rag_with_sources(\"What is Deep Learning?\")\n",
    "\n",
    "print(f\"‚ùì Question: {result['question']}\")\n",
    "print(f\"\\nüí° Answer: {result['answer']}\")\n",
    "print(f\"\\nüìö Sources:\")\n",
    "for i, doc in enumerate(result['sources'], 1):\n",
    "    print(f\"\\n{i}. {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"   {textwrap.fill(doc.page_content, width=70, initial_indent='   ', subsequent_indent='   ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Conversational RAG (15 minutes)\n",
    "\n",
    "Add memory to create a conversational RAG system that remembers context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1: RAG with Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversational RAG chain created\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Keep the answer concise.\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "def get_contextualized_question(input_dict):\n",
    "    if input_dict.get(\"chat_history\"):\n",
    "        return (contextualize_q_prompt | llm | StrOutputParser()).invoke(input_dict)\n",
    "    else:\n",
    "        return input_dict[\"input\"]\n",
    "\n",
    "def retrieve_docs(question):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "conversational_rag = (\n",
    "    RunnablePassthrough.assign(\n",
    "        standalone_question=RunnableLambda(get_contextualized_question)\n",
    "    )\n",
    "    .assign(\n",
    "        context=lambda x: format_docs(retrieve_docs(x[\"standalone_question\"]))\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversational RAG chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2: Have a Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è Starting Conversation:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üë§ User: What is Artificial Intelligence?\n",
      "ü§ñ Assistant: Artificial Intelligence (AI) is the simulation of human intelligence by machines. AI systems can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ User: Can you tell me about the types?\n",
      "ü§ñ Assistant: There are different types of artificial intelligence, including narrow AI (or weak AI) and general AI (or strong AI). Narrow AI is designed for a specific task, while general AI is more flexible and can handle various tasks like a human. Another type is artificial superintelligence, which surpasses human intelligence in all aspects.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ User: Which one is theoretical?\n",
      "ü§ñ Assistant: General AI is the theoretical type of artificial intelligence that aims to replicate human-like intelligence, including the ability to understand, learn, and apply knowledge across different tasks and domains.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ User: How does Machine Learning fit into this?\n",
      "ü§ñ Assistant: Machine Learning is a subset of artificial intelligence that focuses on developing algorithms and statistical models that enable computers to learn from and make predictions or decisions based on data without being explicitly programmed. It is a key technology used in many AI applications to improve performance and accuracy.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "questions_sequence = [\n",
    "    \"What is Artificial Intelligence?\",\n",
    "    \"Can you tell me about the types?\",\n",
    "    \"Which one is theoretical?\",\n",
    "    \"How does Machine Learning fit into this?\"\n",
    "]\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\"üó£Ô∏è Starting Conversation:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for question in questions_sequence:\n",
    "    print(f\"\\nüë§ User: {question}\")\n",
    "    \n",
    "    answer = conversational_rag.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    \n",
    "    chat_history.append(HumanMessage(content=question))\n",
    "    chat_history.append(AIMessage(content=answer))\n",
    "    \n",
    "    print(f\"ü§ñ Assistant: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3: View Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìú Conversation History:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for message in chat_history:\n",
    "    role = \"üë§ User\" if isinstance(message, HumanMessage) else \"ü§ñ Assistant\"\n",
    "    print(f\"\\n{role}: {message.content}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nTotal messages in history: {len(chat_history)}\")\n",
    "print(f\"Number of exchanges: {len(chat_history) // 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Summary & Key Takeaways\n",
    "\n",
    "## What We Learned:\n",
    "\n",
    "### 1. **RAG Fundamentals**\n",
    "- What RAG is and why it's powerful\n",
    "- How RAG solves LLM limitations\n",
    "- RAG architecture and workflow\n",
    "\n",
    "### 2. **Embeddings & Vectors**\n",
    "- Text embeddings capture semantic meaning\n",
    "- Vector similarity measures relatedness\n",
    "- OpenAI embeddings API usage\n",
    "\n",
    "### 3. **PGVector**\n",
    "- Setting up PostgreSQL with PGVector\n",
    "- Storing and searching vectors\n",
    "- Metadata filtering\n",
    "- Similarity search with scores\n",
    "\n",
    "### 4. **Document Processing**\n",
    "- Document loaders for different formats\n",
    "- Text splitting strategies\n",
    "- Chunk size and overlap considerations\n",
    "- Metadata management\n",
    "\n",
    "### 5. **Building RAG Applications**\n",
    "- Complete RAG pipeline with LCEL\n",
    "- Retriever configuration\n",
    "- Source citation\n",
    "- Conversational RAG with memory\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Next Steps:\n",
    "\n",
    "### Exercises for This Week:\n",
    "\n",
    "**Exercise 1 (Due Monday):** `02_exercise_knowledge_base.ipynb`\n",
    "- Build personal knowledge base with RAG\n",
    "- Upload and process multiple documents\n",
    "- Implement Q&A with citations\n",
    "\n",
    "**Exercise 2 (Due Friday):** `03_exercise_research_assistant.ipynb`\n",
    "- Multi-document research assistant\n",
    "- Metadata filtering and organization\n",
    "- Advanced retrieval strategies\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Reflection Questions:\n",
    "\n",
    "1. When should you use RAG vs fine-tuning?\n",
    "2. How does chunk size affect retrieval quality?\n",
    "3. What are the trade-offs of different text splitting strategies?\n",
    "4. How can you improve RAG accuracy?\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources:\n",
    "\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [PGVector Documentation](https://github.com/pgvector/pgvector)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Week:** Introduction to LangGraph! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
