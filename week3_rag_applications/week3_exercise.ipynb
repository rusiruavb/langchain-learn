{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3 - Exercise 1: Personal Knowledge Base\n",
        "\n",
        "## üìã Exercise Overview\n",
        "\n",
        "**Due:** Monday (Week 3)  \n",
        "**Estimated Time:** 3-4 hours  \n",
        "**Difficulty:** Intermediate\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "In this exercise, you will:\n",
        "1. Build a complete RAG system from scratch\n",
        "2. Process and upload multiple documents\n",
        "3. Implement semantic search with PGVector\n",
        "4. Create a Q&A system with source citations\n",
        "5. Add conversation memory for context\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Requirements\n",
        "\n",
        "Your Personal Knowledge Base must:\n",
        "\n",
        "### Core Features:\n",
        "- ‚úÖ **Document Upload:** Load at least 3 different documents (txt, pdf, etc.)\n",
        "- ‚úÖ **Text Splitting:** Implement proper chunking with overlap\n",
        "- ‚úÖ **Vector Storage:** Store embeddings in PGVector\n",
        "- ‚úÖ **Semantic Search:** Retrieve relevant chunks based on query\n",
        "- ‚úÖ **Q&A System:** Answer questions with source citations\n",
        "- ‚úÖ **Conversation Memory:** Maintain context across multiple questions\n",
        "\n",
        "### Technical Requirements:\n",
        "- Use `RecursiveCharacterTextSplitter` with appropriate chunk size\n",
        "- Store metadata (source, chunk_id, timestamp)\n",
        "- Implement similarity search with configurable `k` value\n",
        "- Return source documents with answers\n",
        "- Handle errors gracefully (missing docs, connection issues)\n",
        "\n",
        "### Bonus Challenges (Optional):\n",
        "- üåü Support multiple document formats (PDF, DOCX, TXT)\n",
        "- üåü Implement metadata filtering (by source, date, category)\n",
        "- üåü Add document summarization feature\n",
        "- üåü Create a relevance scoring system\n",
        "- üåü Implement document update/delete functionality\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Hints\n",
        "\n",
        "<details>\n",
        "<summary>Click for Hint 1: Document Processing Pipeline</summary>\n",
        "\n",
        "```python\n",
        "# Pipeline structure:\n",
        "1. Load documents ‚Üí loader.load()\n",
        "2. Split into chunks ‚Üí text_splitter.split_documents()\n",
        "3. Add metadata ‚Üí chunk.metadata.update()\n",
        "4. Create embeddings ‚Üí PGVector.from_documents()\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click for Hint 2: Choosing Chunk Size</summary>\n",
        "\n",
        "Good starting values:\n",
        "- chunk_size: 500-1000 characters\n",
        "- chunk_overlap: 50-200 characters (10-20% of chunk_size)\n",
        "- Experiment to find what works best for your documents\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click for Hint 3: Source Citations</summary>\n",
        "\n",
        "```python\n",
        "# Return both answer and sources\n",
        "retrieved_docs = retriever.get_relevant_documents(question)\n",
        "answer = rag_chain.invoke(question)\n",
        "return {\"answer\": answer, \"sources\": retrieved_docs}\n",
        "```\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import PGVector\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize components\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 1: Create Sample Documents\n",
        "\n",
        "Create at least 3 sample documents for your knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create sample documents\n",
        "# You can create text files manually or use the code below\n",
        "\n",
        "# Example documents\n",
        "doc1_content = \"\"\"\n",
        "# TODO: Add your first document content here\n",
        "# Topic: [Your choice - e.g., Python Programming]\n",
        "\"\"\"\n",
        "\n",
        "doc2_content = \"\"\"\n",
        "# TODO: Add your second document content here\n",
        "# Topic: [Your choice - e.g., Machine Learning]\n",
        "\"\"\"\n",
        "\n",
        "doc3_content = \"\"\"\n",
        "# TODO: Add your third document content here\n",
        "# Topic: [Your choice - e.g., Data Science]\n",
        "\"\"\"\n",
        "\n",
        "# Save documents\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"‚úÖ Sample documents created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 2: Implement DocumentProcessor Class\n",
        "\n",
        "Create a class to handle document loading and processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"\n",
        "    Handles document loading, splitting, and metadata management.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        \"\"\"\n",
        "        Initialize the document processor.\n",
        "        \n",
        "        Args:\n",
        "            chunk_size: Target size for text chunks\n",
        "            chunk_overlap: Overlap between chunks\n",
        "        \"\"\"\n",
        "        # TODO: Initialize text splitter\n",
        "        self.text_splitter = # YOUR CODE HERE\n",
        "        \n",
        "        # Track processed documents\n",
        "        self.processed_docs = []\n",
        "    \n",
        "    def load_document(self, file_path: str, doc_type: str = \"txt\") -> List[Document]:\n",
        "        \"\"\"\n",
        "        Load a document from file.\n",
        "        \n",
        "        Args:\n",
        "            file_path: Path to the document\n",
        "            doc_type: Type of document (txt, pdf)\n",
        "            \n",
        "        Returns:\n",
        "            List of loaded documents\n",
        "        \"\"\"\n",
        "        # TODO: Implement document loading\n",
        "        # Handle different file types\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "    \n",
        "    def process_document(self, file_path: str, category: str = \"general\") -> List[Document]:\n",
        "        \"\"\"\n",
        "        Load, split, and add metadata to a document.\n",
        "        \n",
        "        Args:\n",
        "            file_path: Path to the document\n",
        "            category: Category/tag for the document\n",
        "            \n",
        "        Returns:\n",
        "            List of processed document chunks\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # TODO: Load document\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            # TODO: Split into chunks\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            # TODO: Add metadata to each chunk\n",
        "            # Metadata should include:\n",
        "            # - source (file path)\n",
        "            # - category\n",
        "            # - chunk_id\n",
        "            # - timestamp\n",
        "            # - chunk_size\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            # Track processed documents\n",
        "            self.processed_docs.extend(chunks)\n",
        "            \n",
        "            return chunks\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {file_path}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def get_statistics(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Get processing statistics.\n",
        "        \"\"\"\n",
        "        # TODO: Return statistics about processed documents\n",
        "        # YOUR CODE HERE\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 3: Implement KnowledgeBase Class\n",
        "\n",
        "Create a class to manage the vector store and retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeBase:\n",
        "    \"\"\"\n",
        "    Manages vector store, retrieval, and Q&A functionality.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, collection_name: str = \"personal_kb\"):\n",
        "        \"\"\"\n",
        "        Initialize the knowledge base.\n",
        "        \n",
        "        Args:\n",
        "            collection_name: Name for the PGVector collection\n",
        "        \"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.embeddings = embeddings\n",
        "        self.llm = llm\n",
        "        \n",
        "        # TODO: Initialize connection string\n",
        "        self.connection_string = # YOUR CODE HERE\n",
        "        \n",
        "        # TODO: Initialize vector store (will be set in add_documents)\n",
        "        self.vectorstore = None\n",
        "        self.retriever = None\n",
        "        \n",
        "        # TODO: Initialize conversation memory\n",
        "        self.memory = # YOUR CODE HERE\n",
        "        \n",
        "        # TODO: Create RAG chain\n",
        "        self._create_rag_chain()\n",
        "    \n",
        "    def _create_rag_chain(self):\n",
        "        \"\"\"\n",
        "        Create the RAG chain for Q&A.\n",
        "        \"\"\"\n",
        "        # TODO: Create prompt template\n",
        "        self.rag_prompt = # YOUR CODE HERE\n",
        "        \n",
        "        # Chain will be created after vectorstore is initialized\n",
        "        self.rag_chain = None\n",
        "    \n",
        "    def add_documents(self, documents: List[Document]):\n",
        "        \"\"\"\n",
        "        Add documents to the vector store.\n",
        "        \n",
        "        Args:\n",
        "            documents: List of documents to add\n",
        "        \"\"\"\n",
        "        # TODO: Create or update vector store\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        # TODO: Create retriever\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        # TODO: Create complete RAG chain\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        print(f\"‚úÖ Added {len(documents)} documents to knowledge base\")\n",
        "    \n",
        "    def search(self, query: str, k: int = 3, filter_dict: Dict = None) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Search for relevant documents.\n",
        "        \n",
        "        Args:\n",
        "            query: Search query\n",
        "            k: Number of results to return\n",
        "            filter_dict: Metadata filters\n",
        "            \n",
        "        Returns:\n",
        "            List of relevant documents\n",
        "        \"\"\"\n",
        "        # TODO: Implement search\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "    \n",
        "    def ask(self, question: str, return_sources: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Ask a question and get an answer with sources.\n",
        "        \n",
        "        Args:\n",
        "            question: The question to ask\n",
        "            return_sources: Whether to return source documents\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with answer and sources\n",
        "        \"\"\"\n",
        "        # TODO: Get answer from RAG chain\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        # TODO: Get source documents if requested\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        # TODO: Store in memory\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def get_conversation_history(self) -> List:\n",
        "        \"\"\"\n",
        "        Get the conversation history.\n",
        "        \"\"\"\n",
        "        # TODO: Return conversation history from memory\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "    \n",
        "    def clear_memory(self):\n",
        "        \"\"\"\n",
        "        Clear conversation memory.\n",
        "        \"\"\"\n",
        "        # TODO: Clear memory\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "    \n",
        "    # BONUS: Implement these methods\n",
        "    \n",
        "    def summarize_document(self, source: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a summary of a specific document.\n",
        "        \"\"\"\n",
        "        # TODO (BONUS): Implement document summarization\n",
        "        pass\n",
        "    \n",
        "    def get_statistics(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Get knowledge base statistics.\n",
        "        \"\"\"\n",
        "        # TODO (BONUS): Return statistics\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Testing Your Implementation\n",
        "\n",
        "Run these tests to verify your knowledge base works correctly:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 1: Document Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Test 1: Document Processing\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize processor\n",
        "processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "# Process documents\n",
        "all_chunks = []\n",
        "documents = [\n",
        "    (\"doc1.txt\", \"programming\"),\n",
        "    (\"doc2.txt\", \"machine-learning\"),\n",
        "    (\"doc3.txt\", \"data-science\")\n",
        "]\n",
        "\n",
        "for file_path, category in documents:\n",
        "    chunks = processor.process_document(file_path, category)\n",
        "    all_chunks.extend(chunks)\n",
        "    print(f\"‚úÖ Processed {file_path}: {len(chunks)} chunks\")\n",
        "\n",
        "print(f\"\\nüìä Total chunks: {len(all_chunks)}\")\n",
        "\n",
        "# Show statistics\n",
        "stats = processor.get_statistics()\n",
        "print(f\"\\nüìä Statistics:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# ‚úÖ Should successfully process all documents!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Knowledge Base Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 2: Knowledge Base Creation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create knowledge base\n",
        "kb = KnowledgeBase(collection_name=\"test_kb\")\n",
        "\n",
        "# Add documents\n",
        "kb.add_documents(all_chunks)\n",
        "\n",
        "print(\"‚úÖ Knowledge base created successfully\")\n",
        "\n",
        "# ‚úÖ Should create vector store without errors!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: Semantic Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 3: Semantic Search\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Search for relevant documents\n",
        "query = \"What is machine learning?\"\n",
        "results = kb.search(query, k=3)\n",
        "\n",
        "print(f\"üîç Query: {query}\")\n",
        "print(f\"\\nüìö Found {len(results)} relevant chunks:\\n\")\n",
        "\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"{i}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"   Category: {doc.metadata.get('category', 'Unknown')}\")\n",
        "    print(f\"   Content: {doc.page_content[:100]}...\")\n",
        "    print()\n",
        "\n",
        "# ‚úÖ Should return relevant chunks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 4: Q&A with Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 4: Q&A with Sources\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "questions = [\n",
        "    \"What is Python used for?\",\n",
        "    \"Explain machine learning in simple terms.\",\n",
        "    \"What is data science?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"\\n‚ùì Question: {question}\")\n",
        "    \n",
        "    result = kb.ask(question, return_sources=True)\n",
        "    \n",
        "    print(f\"üí° Answer: {result['answer']}\")\n",
        "    \n",
        "    if 'sources' in result:\n",
        "        print(f\"\\nüìö Sources:\")\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"  {i}. {source.metadata.get('source', 'Unknown')}\")\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ‚úÖ Should provide accurate answers with sources!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 5: Conversational Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 5: Conversational Memory\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear previous memory\n",
        "kb.clear_memory()\n",
        "\n",
        "# Have a conversation\n",
        "conversation = [\n",
        "    \"What is Python?\",\n",
        "    \"What are its main features?\",  # Should reference Python\n",
        "    \"How is it used in data science?\"  # Should maintain context\n",
        "]\n",
        "\n",
        "for question in conversation:\n",
        "    print(f\"\\nüë§ User: {question}\")\n",
        "    result = kb.ask(question, return_sources=False)\n",
        "    print(f\"ü§ñ Assistant: {result['answer']}\")\n",
        "\n",
        "# View conversation history\n",
        "print(\"\\nüìú Conversation History:\")\n",
        "history = kb.get_conversation_history()\n",
        "print(f\"Total exchanges: {len(history)}\")\n",
        "\n",
        "# ‚úÖ Should maintain context across questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 6: Metadata Filtering (Bonus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 6: Metadata Filtering\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Search with category filter\n",
        "filtered_results = kb.search(\n",
        "    query=\"programming\",\n",
        "    k=5,\n",
        "    filter_dict={\"category\": \"programming\"}\n",
        ")\n",
        "\n",
        "print(f\"üîç Filtered search (category='programming'):\")\n",
        "print(f\"Found {len(filtered_results)} results\")\n",
        "\n",
        "for doc in filtered_results:\n",
        "    print(f\"  - {doc.metadata.get('source')}: {doc.metadata.get('category')}\")\n",
        "\n",
        "# ‚úÖ Should only return documents from specified category!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üé® Your Own Tests\n",
        "\n",
        "Add your own test cases here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR TEST CASES HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Self-Assessment\n",
        "\n",
        "Rate your implementation (1-5):\n",
        "\n",
        "| Criteria | Rating | Notes |\n",
        "|----------|--------|-------|\n",
        "| Document Processing | /5 | Loads and splits correctly? |\n",
        "| Vector Store | /5 | PGVector integration works? |\n",
        "| Semantic Search | /5 | Returns relevant results? |\n",
        "| Q&A Accuracy | /5 | Provides correct answers? |\n",
        "| Source Citations | /5 | Properly cites sources? |\n",
        "| Conversation Memory | /5 | Maintains context? |\n",
        "| Error Handling | /5 | Handles errors gracefully? |\n",
        "| Code Quality | /5 | Clean, documented code? |\n",
        "| Bonus Features | /5 | Extra features implemented? |\n",
        "| **Total** | **/45** | |\n",
        "\n",
        "---\n",
        "\n",
        "## ü§î Reflection Questions\n",
        "\n",
        "Answer these questions in the markdown cell below:\n",
        "\n",
        "1. What chunk size and overlap worked best for your documents? Why?\n",
        "2. How did you handle documents with different structures?\n",
        "3. What strategies did you use to improve retrieval accuracy?\n",
        "4. How would you scale this to thousands of documents?\n",
        "5. What metadata proved most useful for filtering?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Answers:\n",
        "\n",
        "**1. Chunk Size Selection:**\n",
        "- [Your answer here]\n",
        "\n",
        "**2. Handling Different Document Structures:**\n",
        "- [Your answer here]\n",
        "\n",
        "**3. Improving Retrieval Accuracy:**\n",
        "- [Your answer here]\n",
        "\n",
        "**4. Scaling Strategy:**\n",
        "- [Your answer here]\n",
        "\n",
        "**5. Useful Metadata:**\n",
        "- [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì§ Submission\n",
        "\n",
        "### Before Submitting:\n",
        "\n",
        "- [ ] All tests pass\n",
        "- [ ] Documents are properly processed and stored\n",
        "- [ ] Semantic search returns relevant results\n",
        "- [ ] Q&A provides accurate answers with sources\n",
        "- [ ] Conversation memory works correctly\n",
        "- [ ] Code is well-documented\n",
        "- [ ] Error handling is implemented\n",
        "- [ ] Reflection questions answered\n",
        "- [ ] Notebook runs from top to bottom without errors\n",
        "\n",
        "### How to Submit:\n",
        "\n",
        "1. Save this notebook\n",
        "2. Commit to your git branch: `git commit -m \"Complete Week 3 Exercise 1\"`\n",
        "3. Push to repository: `git push origin week3-exercise1`\n",
        "4. Submit repository link to instructor\n",
        "\n",
        "---\n",
        "\n",
        "**Excellent work on building your RAG system! üéâ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
