{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: LangChain Core Concepts\n",
    "\n",
    "## üìö Session Overview\n",
    "\n",
    "**Duration:** 2 hours  \n",
    "**Week:** 2  \n",
    "**Instructor-Led Session**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. Understand the LangChain framework and its core abstractions\n",
    "2. Work with Chat Models, Prompts, and Output Parsers\n",
    "3. Build chains using LangChain Expression Language (LCEL)\n",
    "4. Implement different types of memory for conversations\n",
    "5. Compose complex workflows using chain components\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- ‚úÖ Completed Week 1 (LLM Fundamentals)\n",
    "- ‚úÖ Understanding of basic chatbot concepts\n",
    "- ‚úÖ LangChain installed (`pip install langchain langchain-openai`)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "\n",
    "- Setup & Introduction: 10 minutes\n",
    "- Section 1 (LangChain Intro): 25 minutes\n",
    "- Section 2 (Core Components): 40 minutes\n",
    "- Section 3 (LCEL & Chains): 30 minutes\n",
    "- Section 4 (Memory): 20 minutes\n",
    "- Wrap-up & Q&A: 5 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Standard imports\n",
    "from typing import Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"üì¶ LangChain imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Introduction to LangChain\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "LangChain is a **framework** for developing applications powered by language models.\n",
    "\n",
    "### ü§î Why LangChain?\n",
    "\n",
    "**Without LangChain** (Week 1 approach):\n",
    "- Manual message management\n",
    "- Custom prompt formatting\n",
    "- Manual chain creation\n",
    "- No standardized interfaces\n",
    "\n",
    "**With LangChain**:\n",
    "- ‚úÖ Standardized components\n",
    "- ‚úÖ Easy chain composition\n",
    "- ‚úÖ Built-in memory management\n",
    "- ‚úÖ Reusable prompt templates\n",
    "- ‚úÖ Rich ecosystem of integrations\n",
    "\n",
    "---\n",
    "\n",
    "## Core Abstractions\n",
    "\n",
    "LangChain has several key components:\n",
    "\n",
    "### 1. **Models**\n",
    "- Chat Models (ChatOpenAI, ChatAnthropic)\n",
    "- LLMs (OpenAI, HuggingFace)\n",
    "- Embedding Models\n",
    "\n",
    "### 2. **Prompts**\n",
    "- Prompt Templates\n",
    "- Chat Prompt Templates\n",
    "- Few-shot examples\n",
    "\n",
    "### 3. **Output Parsers**\n",
    "- String Parser\n",
    "- JSON Parser\n",
    "- Pydantic Parser (structured outputs)\n",
    "\n",
    "### 4. **Chains**\n",
    "- Simple chains\n",
    "- Sequential chains\n",
    "- LCEL (LangChain Expression Language)\n",
    "\n",
    "### 5. **Memory**\n",
    "- Conversation Buffer Memory\n",
    "- Conversation Summary Memory\n",
    "- Custom memory implementations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Your First LangChain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a chat model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Simple invocation\n",
    "response = llm.invoke(\"What is LangChain?\")\n",
    "\n",
    "print(\"ü§ñ Response:\")\n",
    "print(response.content)\n",
    "print()\n",
    "print(\"üìä Response Type:\", type(response))\n",
    "print(\"üìä Content Type:\", type(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain uses message objects\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Explain quantum computing in simple terms.\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"ü§ñ Response:\")\n",
    "print(response.content)\n",
    "\n",
    "# You can also add AI messages\n",
    "messages.append(AIMessage(content=response.content))\n",
    "messages.append(HumanMessage(content=\"Give me a real-world example.\"))\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"\\nü§ñ Follow-up Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Core Components Deep Dive\n",
    "\n",
    "Let's explore each core component in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Prompt Templates\n",
    "\n",
    "Prompt templates make it easy to create reusable prompts with variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a template with variables\n",
    "template = \"\"\"You are a {role}.\n",
    "Please {task} about {topic}.\n",
    "Keep your response under {word_limit} words.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"role\", \"task\", \"topic\", \"word_limit\"]\n",
    ")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = prompt.format(\n",
    "    role=\"science teacher\",\n",
    "    task=\"explain\",\n",
    "    topic=\"photosynthesis\",\n",
    "    word_limit=50\n",
    ")\n",
    "\n",
    "print(\"üìù Formatted Prompt:\")\n",
    "print(formatted_prompt)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Use with LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(\"ü§ñ Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Templates\n",
    "\n",
    "For chat models, use `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat prompt template with multiple messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {personality} assistant.\"),\n",
    "    (\"human\", \"Hi, my name is {name}.\"),\n",
    "    (\"ai\", \"Hello {name}! Nice to meet you. How can I help you today?\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# Format the messages\n",
    "messages = chat_prompt.format_messages(\n",
    "    personality=\"friendly\",\n",
    "    name=\"Alex\",\n",
    "    user_input=\"Tell me about yourself.\"\n",
    ")\n",
    "\n",
    "print(\"üìù Formatted Messages:\")\n",
    "for msg in messages:\n",
    "    print(f\"[{msg.__class__.__name__}]: {msg.content}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Use with LLM\n",
    "response = llm.invoke(messages)\n",
    "print(\"ü§ñ Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Try It Yourself!\n",
    "\n",
    "**Exercise:** Create a prompt template for a translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a translation prompt template with variables:\n",
    "# - source_language\n",
    "# - target_language\n",
    "# - text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Output Parsers\n",
    "\n",
    "Output parsers help structure the LLM's response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String output parser (most common)\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "# Without parser\n",
    "response_without_parser = llm.invoke(\"Say hello\")\n",
    "print(\"Without Parser:\")\n",
    "print(f\"Type: {type(response_without_parser)}\")\n",
    "print(f\"Content: {response_without_parser.content}\")\n",
    "print()\n",
    "\n",
    "# With parser\n",
    "response_with_parser = str_parser.invoke(response_without_parser)\n",
    "print(\"With Parser:\")\n",
    "print(f\"Type: {type(response_with_parser)}\")\n",
    "print(f\"Content: {response_with_parser}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON output parser for structured data\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "# Create a prompt that asks for JSON\n",
    "json_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Always respond with valid JSON.\"),\n",
    "    (\"human\", \"\"\"Analyze the sentiment of this text and return a JSON object with:\n",
    "    - sentiment: positive, negative, or neutral\n",
    "    - confidence: a number from 0 to 1\n",
    "    - key_phrases: list of important phrases\n",
    "    \n",
    "    Text: {text}\n",
    "    \n",
    "    Return only the JSON, no other text.\"\"\")\n",
    "])\n",
    "\n",
    "# Format and invoke\n",
    "messages = json_prompt.format_messages(text=\"I absolutely love this product! It exceeded all my expectations.\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"Raw Response:\")\n",
    "print(response.content)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Parse JSON\n",
    "try:\n",
    "    parsed = json_parser.parse(response.content)\n",
    "    print(\"Parsed JSON:\")\n",
    "    print(f\"Type: {type(parsed)}\")\n",
    "    print(f\"Sentiment: {parsed.get('sentiment')}\")\n",
    "    print(f\"Confidence: {parsed.get('confidence')}\")\n",
    "    print(f\"Key Phrases: {parsed.get('key_phrases')}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Parsing error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic Output Parser (Structured Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Define the structure using Pydantic\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str = Field(description=\"Person's full name\")\n",
    "    age: int = Field(description=\"Person's age\")\n",
    "    occupation: str = Field(description=\"Person's job or occupation\")\n",
    "    hobbies: List[str] = Field(description=\"List of hobbies\")\n",
    "\n",
    "# Create parser\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=PersonInfo)\n",
    "\n",
    "# Create prompt with format instructions\n",
    "pydantic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract person information from the text.\"),\n",
    "    (\"human\", \"\"\"Extract the person's information from this text:\n",
    "    \n",
    "    {text}\n",
    "    \n",
    "    {format_instructions}\"\"\")\n",
    "])\n",
    "\n",
    "# Get format instructions\n",
    "format_instructions = pydantic_parser.get_format_instructions()\n",
    "\n",
    "# Format prompt\n",
    "messages = pydantic_prompt.format_messages(\n",
    "    text=\"John Smith is a 32-year-old software engineer who enjoys hiking, photography, and playing guitar.\",\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "# Get response and parse\n",
    "response = llm.invoke(messages)\n",
    "person = pydantic_parser.parse(response.content)\n",
    "\n",
    "print(\"‚úÖ Parsed Person Information:\")\n",
    "print(f\"Type: {type(person)}\")\n",
    "print(f\"Name: {person.name}\")\n",
    "print(f\"Age: {person.age}\")\n",
    "print(f\"Occupation: {person.occupation}\")\n",
    "print(f\"Hobbies: {', '.join(person.hobbies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: LCEL & Chain Composition\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is a declarative way to compose chains.\n",
    "\n",
    "**Theory:**\n",
    "LangChain Expression Language (LCEL) uses the pipe operator `|` to compose components into chains. This is inspired by Unix pipes where the output of one command becomes the input to the next.\n",
    "\n",
    "**How it works:**\n",
    "```python\n",
    "chain = component1 | component2 | component3\n",
    "result = chain.invoke(input)\n",
    "```\n",
    "\n",
    "Is equivalent to:\n",
    "```python\n",
    "step1 = component1(input)\n",
    "step2 = component2(step1)\n",
    "result = component3(step2)\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- Readability: Clear data flow from left to right\n",
    "- Composability: Easy to add/remove/reorder steps\n",
    "- Declarative: Describe what to do, not how\n",
    "- Debugging: Each component can be tested independently\n",
    "\n",
    "## Key Concept: The Pipe Operator (`|`)\n",
    "\n",
    "LCEL uses the pipe operator to chain components:\n",
    "```python\n",
    "chain = prompt | model | parser\n",
    "```\n",
    "\n",
    "This is equivalent to:\n",
    "```python\n",
    "result = parser(model(prompt(input)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chain: prompt -> model -> parser\n",
    "simple_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a {length} joke about {topic}.\"\n",
    ")\n",
    "\n",
    "# Compose the chain\n",
    "simple_chain = simple_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "result = simple_chain.invoke({\n",
    "    \"length\": \"short\",\n",
    "    \"topic\": \"programming\"\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Result:\")\n",
    "print(result)\n",
    "print(\"\\nüìä Type:\", type(result))  # Now it's a string!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Sequential Chain\n",
    "\n",
    "Chains can be composed sequentially for multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate a topic\n",
    "topic_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Suggest a creative topic for a {genre} story. Return only the topic, one sentence.\"\n",
    ")\n",
    "topic_chain = topic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: Write the story\n",
    "story_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a short {genre} story (3-4 sentences) about: {topic}\"\n",
    ")\n",
    "story_chain = story_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 3: Analyze the story\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze this story and provide: mood, key themes, and a rating (1-10).\\n\\nStory: {story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Execute the pipeline\n",
    "genre = \"science fiction\"\n",
    "\n",
    "print(\"üìñ Generating story...\\n\")\n",
    "\n",
    "# Step 1\n",
    "topic = topic_chain.invoke({\"genre\": genre})\n",
    "print(f\"üìå Topic: {topic}\")\n",
    "print()\n",
    "\n",
    "# Step 2\n",
    "story = story_chain.invoke({\"genre\": genre, \"topic\": topic})\n",
    "print(f\"üìù Story:\\n{story}\")\n",
    "print()\n",
    "\n",
    "# Step 3\n",
    "analysis = analysis_chain.invoke({\"story\": story})\n",
    "print(f\"üîç Analysis:\\n{analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: RunnablePassthrough\n",
    "\n",
    "**Theory:**\n",
    "`RunnablePassthrough` allows data to flow through a chain while also being used by other components. Think of it as a \"splitter\" that keeps the original data while also processing it.\n",
    "\n",
    "**Two main uses:**\n",
    "\n",
    "1. **Pass data unchanged:**\n",
    "```python\n",
    "RunnablePassthrough()  # Input passes through as-is\n",
    "```\n",
    "\n",
    "2. **Assign new fields to dict:**\n",
    "```python\n",
    "RunnablePassthrough.assign(\n",
    "    new_field=some_chain\n",
    ")\n",
    "```\n",
    "\n",
    "**Example flow:**\n",
    "```python\n",
    "Input: {\"text\": \"Hello\"}\n",
    "‚Üì\n",
    "RunnablePassthrough.assign(translation=translate_chain)\n",
    "‚Üì\n",
    "Output: {\"text\": \"Hello\", \"translation\": \"Hola\"}\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- Preserve context: Keep original data while adding new information\n",
    "- Build complex objects: Gradually construct result dictionaries\n",
    "- Parallel operations: Run multiple chains on the same input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Translate and also keep the original\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate this to {language}: {text}\"\n",
    ")\n",
    "\n",
    "# Chain that returns both original and translation\n",
    "translation_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        translation=(translate_prompt | llm | StrOutputParser())\n",
    "    )\n",
    ")\n",
    "\n",
    "result = translation_chain.invoke({\n",
    "    \"text\": \"Hello, how are you?\",\n",
    "    \"language\": \"Spanish\"\n",
    "})\n",
    "\n",
    "print(\"üìù Result:\")\n",
    "print(f\"Original: {result['text']}\")\n",
    "print(f\"Translation: {result['translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4: RunnableLambda\n",
    "\n",
    "**Theory:**\n",
    "`RunnableLambda` lets you use custom Python functions inside chains. This bridges LangChain components with your own logic.\n",
    "\n",
    "**When to use:**\n",
    "- Custom data transformation\n",
    "- API calls to external services\n",
    "- Data validation\n",
    "- Format conversion\n",
    "- Business logic\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "def word_count(text: str) -> dict:\n",
    "    return {\"text\": text, \"count\": len(text.split())}\n",
    "\n",
    "chain = prompt | llm | StrOutputParser() | RunnableLambda(word_count)\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- Flexibility: Integrate any Python code\n",
    "- No limitations: Not restricted to LangChain components\n",
    "- Reusability: Use existing functions in chains\n",
    "- Testing: Easy to test functions independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to process text\n",
    "def word_count(text: str) -> Dict:\n",
    "    \"\"\"Count words in text.\"\"\"\n",
    "    words = text.split()\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"word_count\": len(words),\n",
    "        \"char_count\": len(text)\n",
    "    }\n",
    "\n",
    "# Create chain with custom function\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this in one sentence: {text}\"\n",
    ")\n",
    "\n",
    "summary_chain = (\n",
    "    summary_prompt \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | RunnableLambda(word_count)  # Apply custom function\n",
    ")\n",
    "\n",
    "result = summary_chain.invoke({\n",
    "    \"text\": \"\"\"Artificial intelligence is transforming industries by automating tasks, \n",
    "    providing insights from data, and enabling new capabilities. Machine learning, \n",
    "    a subset of AI, allows systems to learn from data without explicit programming.\"\"\"\n",
    "})\n",
    "\n",
    "print(\"üìä Summary Analysis:\")\n",
    "print(f\"Summary: {result['text']}\")\n",
    "print(f\"Word Count: {result['word_count']}\")\n",
    "print(f\"Character Count: {result['char_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5: RunnableParallel\n",
    "\n",
    "**Theory:**\n",
    "`RunnableParallel` executes multiple chains simultaneously on the same input and combines results into a dictionary.\n",
    "\n",
    "**How it works:**\n",
    "```python\n",
    "parallel = RunnableParallel(\n",
    "    sentiment=sentiment_chain,\n",
    "    summary=summary_chain,\n",
    "    keywords=keywords_chain\n",
    ")\n",
    "result = parallel.invoke({\"text\": \"...\"})  \n",
    "# ‚Üí {\"sentiment\": \"...\", \"summary\": \"...\", \"keywords\": [...]}\n",
    "```\n",
    "\n",
    "**Visual representation:**\n",
    "```\n",
    "Input\n",
    "  ‚îú‚îÄ‚îÄ Chain A ‚Üí result_a\n",
    "  ‚îú‚îÄ‚îÄ Chain B ‚Üí result_b\n",
    "  ‚îî‚îÄ‚îÄ Chain C ‚Üí result_c\n",
    "       ‚Üì\n",
    "{\"a\": result_a, \"b\": result_b, \"c\": result_c}\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- Performance: Run independent operations concurrently\n",
    "- Efficiency: Save time on parallel-safe operations\n",
    "- Organization: Group related analyses together\n",
    "- Comprehensive results: Get multiple perspectives at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple chains for different analyses\n",
    "sentiment_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the sentiment of this text (positive/negative/neutral)? Text: {text}. Answer with just one word.\"\n",
    ")\n",
    "sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
    "\n",
    "language_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is this text in? Text: {text}. Answer with just the language name.\"\n",
    ")\n",
    "language_chain = language_prompt | llm | StrOutputParser()\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize in 5 words: {text}\"\n",
    ")\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine chains in parallel\n",
    "parallel_chain = RunnableParallel(\n",
    "    sentiment=sentiment_chain,\n",
    "    language=language_chain,\n",
    "    summary=summary_chain\n",
    ")\n",
    "\n",
    "# Execute all chains simultaneously\n",
    "result = parallel_chain.invoke({\n",
    "    \"text\": \"I love learning about artificial intelligence! It's fascinating.\"\n",
    "})\n",
    "\n",
    "print(\"üîÑ Parallel Analysis Results:\")\n",
    "print(f\"Sentiment: {result['sentiment']}\")\n",
    "print(f\"Language: {result['language']}\")\n",
    "print(f\"Summary: {result['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Try It Yourself!\n",
    "\n",
    "**Exercise:** Create a chain that translates text to multiple languages in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create chains that translate to Spanish, French, and German in parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Memory Management\n",
    "\n",
    "LangChain provides built-in memory classes for conversation management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: ConversationBufferMemory\n",
    "\n",
    "Stores all messages in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"demo_session\"}}\n",
    "\n",
    "# Have a conversation\n",
    "print(\"Conversation 1:\")\n",
    "response1 = conversation.invoke({\"input\": \"Hi, my name is Alex.\"}, config=config)\n",
    "print(f\"AI: {response1}\")\n",
    "print()\n",
    "\n",
    "print(\"Conversation 2:\")\n",
    "response2 = conversation.invoke({\"input\": \"What's my name?\"}, config=config)\n",
    "print(f\"AI: {response2}\")\n",
    "print()\n",
    "\n",
    "# View memory\n",
    "print(\"\\nüìú Memory Contents:\")\n",
    "history = get_session_history(\"demo_session\")\n",
    "print(f\"Messages: {history.messages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: ConversationBufferWindowMemory\n",
    "\n",
    "Stores only the last K messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedChatHistory(BaseChatMessageHistory):\n",
    "    def __init__(self, k: int = 2):\n",
    "        self.messages: List = []\n",
    "        self.k = k\n",
    "    \n",
    "    def add_message(self, message):\n",
    "        self.messages.append(message)\n",
    "        if len(self.messages) > self.k * 2:\n",
    "            self.messages = self.messages[-self.k * 2:]\n",
    "    \n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "\n",
    "window_store = {}\n",
    "\n",
    "def get_windowed_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in window_store:\n",
    "        window_store[session_id] = WindowedChatHistory(k=2)\n",
    "    return window_store[session_id]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "conversation_window = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_windowed_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "window_config = {\"configurable\": {\"session_id\": \"window_session\"}}\n",
    "\n",
    "# Have multiple conversations\n",
    "conversation_window.invoke({\"input\": \"My favorite color is blue.\"}, config=window_config)\n",
    "conversation_window.invoke({\"input\": \"I have a dog named Max.\"}, config=window_config)\n",
    "conversation_window.invoke({\"input\": \"I work as a software engineer.\"}, config=window_config)\n",
    "conversation_window.invoke({\"input\": \"I love hiking on weekends.\"}, config=window_config)\n",
    "\n",
    "# Test memory\n",
    "print(\"Testing memory with window size of 2...\\n\")\n",
    "response = conversation_window.invoke({\"input\": \"What's my favorite color?\"}, config=window_config)\n",
    "print(f\"Question: What's my favorite color?\")\n",
    "print(f\"AI: {response}\")\n",
    "print()\n",
    "\n",
    "# View what's in memory\n",
    "print(\"üìú Current Memory (last 2 interactions):\")\n",
    "history = get_windowed_history(\"window_session\")\n",
    "print(f\"Number of messages: {len(history.messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3: ConversationSummaryMemory\n",
    "\n",
    "Automatically summarizes the conversation to save tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryChatHistory(BaseChatMessageHistory):\n",
    "    def __init__(self, llm, max_messages: int = 6):\n",
    "        self.messages: List = []\n",
    "        self.llm = llm\n",
    "        self.max_messages = max_messages\n",
    "        self.summary = \"\"\n",
    "    \n",
    "    def add_message(self, message):\n",
    "        self.messages.append(message)\n",
    "        if len(self.messages) > self.max_messages:\n",
    "            self._summarize_and_trim()\n",
    "    \n",
    "    def _summarize_and_trim(self):\n",
    "        messages_to_summarize = self.messages[:-2]\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{m.type}: {m.content}\" for m in messages_to_summarize\n",
    "        ])\n",
    "        \n",
    "        summary_prompt = f\"\"\"Summarize the following conversation concisely:\n",
    "\n",
    "{conversation_text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        \n",
    "        summary_response = self.llm.invoke(summary_prompt)\n",
    "        self.summary = summary_response.content\n",
    "        self.messages = self.messages[-2:]\n",
    "    \n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "        self.summary = \"\"\n",
    "\n",
    "summary_store = {}\n",
    "\n",
    "def get_summary_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in summary_store:\n",
    "        summary_store[session_id] = SummaryChatHistory(llm)\n",
    "    return summary_store[session_id]\n",
    "\n",
    "prompt_with_summary = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Previous conversation summary: {summary}\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "def add_summary(inputs):\n",
    "    history = get_summary_history(\"summary_session\")\n",
    "    return {\n",
    "        \"input\": inputs[\"input\"],\n",
    "        \"history\": history.messages,\n",
    "        \"summary\": history.summary or \"No previous conversation\"\n",
    "    }\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(add_summary)\n",
    "    | prompt_with_summary\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "conversation_summary = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_summary_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "summary_config = {\"configurable\": {\"session_id\": \"summary_session\"}}\n",
    "\n",
    "# Have a longer conversation\n",
    "conversation_summary.invoke({\"input\": \"Hi! I'm learning about AI and machine learning.\"}, config=summary_config)\n",
    "conversation_summary.invoke({\"input\": \"I'm particularly interested in natural language processing.\"}, config=summary_config)\n",
    "conversation_summary.invoke({\"input\": \"I've been working on a chatbot project using Python.\"}, config=summary_config)\n",
    "conversation_summary.invoke({\"input\": \"The chatbot helps users find information quickly.\"}, config=summary_config)\n",
    "\n",
    "# View the summary\n",
    "print(\"üìú Conversation Summary:\")\n",
    "history = get_summary_history(\"summary_session\")\n",
    "if history.summary:\n",
    "    print(f\"Summary: {history.summary}\")\n",
    "print(f\"Recent messages: {len(history.messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4: Custom Memory with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom conversational chain with LCEL\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# Store for conversation histories\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create prompt with message placeholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Wrap with message history\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Have a conversation\n",
    "config = {\"configurable\": {\"session_id\": \"user123\"}}\n",
    "\n",
    "response1 = conversational_chain.invoke(\n",
    "    {\"input\": \"My name is Sarah.\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"AI: {response1}\")\n",
    "print()\n",
    "\n",
    "response2 = conversational_chain.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"AI: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Summary & Key Takeaways\n",
    "\n",
    "## What We Learned:\n",
    "\n",
    "### 1. **LangChain Framework**\n",
    "- Why LangChain matters for AI applications\n",
    "- Core abstractions and components\n",
    "- Standardized interfaces for LLMs\n",
    "\n",
    "### 2. **Prompt Templates**\n",
    "- Creating reusable prompts with variables\n",
    "- ChatPromptTemplate for chat models\n",
    "- MessagesPlaceholder for dynamic conversations\n",
    "\n",
    "### 3. **Output Parsers**\n",
    "- StrOutputParser for simple text\n",
    "- JsonOutputParser for structured data\n",
    "- PydanticOutputParser for type-safe outputs\n",
    "\n",
    "### 4. **LCEL (LangChain Expression Language)**\n",
    "- Pipe operator (`|`) for chain composition\n",
    "- RunnablePassthrough for data flow\n",
    "- RunnableLambda for custom functions\n",
    "- RunnableParallel for concurrent execution\n",
    "\n",
    "### 5. **Memory Management**\n",
    "- ConversationBufferMemory for full history\n",
    "- ConversationBufferWindowMemory for recent messages\n",
    "- ConversationSummaryMemory for token efficiency\n",
    "- Custom memory with RunnableWithMessageHistory\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Next Steps:\n",
    "\n",
    "### Exercises for This Week:\n",
    "\n",
    "**Exercise 1 (Due Monday):** `02_exercise_email_assistant.ipynb`\n",
    "- Build multi-step email processing chain\n",
    "- Implement structured outputs\n",
    "- Use LCEL composition\n",
    "\n",
    "**Exercise 2 (Due Friday):** `03_exercise_multilang_processor.ipynb`\n",
    "- Create translation and analysis pipeline\n",
    "- Implement parallel processing\n",
    "- Add error handling\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Reflection Questions:\n",
    "\n",
    "1. How is LangChain different from direct API calls?\n",
    "2. When would you use parallel chains vs sequential chains?\n",
    "3. What are the trade-offs between different memory types?\n",
    "4. How can LCEL improve code readability?\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources:\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LCEL Guide](https://python.langchain.com/docs/expression_language/)\n",
    "- [Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Week:** RAG (Retrieval-Augmented Generation) with PGVector! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
