{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 - Exercise 1: Content Moderation Pipeline\n",
        "\n",
        "## üìã Exercise Overview\n",
        "\n",
        "**Due:** Monday (Week 4)  \n",
        "**Estimated Time:** 3-4 hours  \n",
        "**Difficulty:** Intermediate\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "In this exercise, you will:\n",
        "1. Build a multi-stage content moderation system\n",
        "2. Implement parallel checks for different violation types\n",
        "3. Use conditional routing for decision-making\n",
        "4. Create human-in-the-loop approval flows\n",
        "5. Track moderation decisions and reasons\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Requirements\n",
        "\n",
        "Your Content Moderation Pipeline must:\n",
        "\n",
        "### Core Features:\n",
        "- ‚úÖ **Initial Classification:** Determine content type (text, comment, post)\n",
        "- ‚úÖ **Parallel Checks:** Run multiple moderation checks simultaneously\n",
        "  - Toxicity detection\n",
        "  - Spam detection\n",
        "  - PII (Personal Identifiable Information) detection\n",
        "- ‚úÖ **Decision Logic:** Approve, flag for review, or reject based on checks\n",
        "- ‚úÖ **Human Review:** For flagged content, route to human reviewer\n",
        "- ‚úÖ **Audit Trail:** Track all decisions and reasons\n",
        "\n",
        "### Technical Requirements:\n",
        "- Use TypedDict for state management\n",
        "- Implement at least 3 parallel moderation checks\n",
        "- Use conditional edges for routing\n",
        "- Store moderation metadata (timestamps, scores, reasons)\n",
        "- Handle edge cases (empty content, very long content)\n",
        "\n",
        "### Bonus Challenges (Optional):\n",
        "- üåü Add confidence scores to moderation decisions\n",
        "- üåü Implement escalation for multiple violations\n",
        "- üåü Add content category-specific rules\n",
        "- üåü Create moderation statistics dashboard\n",
        "- üåü Implement appeal process for rejected content\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Hints\n",
        "\n",
        "<details>\n",
        "<summary>Click for Hint 1: Parallel Checks Structure</summary>\n",
        "\n",
        "```python\n",
        "# Create separate nodes for each check\n",
        "workflow.add_node(\"toxicity_check\", check_toxicity)\n",
        "workflow.add_node(\"spam_check\", check_spam)\n",
        "workflow.add_node(\"pii_check\", check_pii)\n",
        "\n",
        "# They can all run after classification\n",
        "workflow.add_edge(\"classify\", \"toxicity_check\")\n",
        "workflow.add_edge(\"classify\", \"spam_check\")\n",
        "workflow.add_edge(\"classify\", \"pii_check\")\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click for Hint 2: Decision Logic</summary>\n",
        "\n",
        "```python\n",
        "def make_decision(state):\n",
        "    violations = 0\n",
        "    if state[\"toxicity_score\"] > 0.7:\n",
        "        violations += 1\n",
        "    if state[\"spam_score\"] > 0.8:\n",
        "        violations += 1\n",
        "    \n",
        "    if violations >= 2:\n",
        "        return \"reject\"\n",
        "    elif violations == 1:\n",
        "        return \"review\"\n",
        "    else:\n",
        "        return \"approve\"\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Click for Hint 3: State with Reducers</summary>\n",
        "\n",
        "```python\n",
        "class ModerationState(TypedDict):\n",
        "    content: str\n",
        "    violations: Annotated[list[str], operator.add]\n",
        "    toxicity_score: float\n",
        "    spam_score: float\n",
        "    decision: str\n",
        "```\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 1: Define State Schema\n",
        "\n",
        "Create the state structure for content moderation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define the ModerationState TypedDict\n",
        "class ModerationState(TypedDict):\n",
        "    \"\"\"State for content moderation pipeline.\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # Include fields for:\n",
        "    # - content (str): The content to moderate\n",
        "    # - content_type (str): Type of content (text/comment/post)\n",
        "    # - toxicity_score (float): Score from toxicity check\n",
        "    # - spam_score (float): Score from spam check\n",
        "    # - pii_detected (bool): Whether PII was found\n",
        "    # - violations (list with reducer): List of violation reasons\n",
        "    # - decision (str): Final decision (approve/review/reject)\n",
        "    # - timestamp (str): When moderation occurred\n",
        "    # - human_review_required (bool): Whether human review is needed\n",
        "    pass\n",
        "\n",
        "print(\"‚úÖ State schema defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 2: Implement Classification Node\n",
        "\n",
        "Classify the type of content being moderated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_content(state: ModerationState) -> ModerationState:\n",
        "    \"\"\"\n",
        "    Classify the content type.\n",
        "    \n",
        "    Args:\n",
        "        state: Current moderation state\n",
        "        \n",
        "    Returns:\n",
        "        Updated state with content_type\n",
        "    \"\"\"\n",
        "    # TODO: Use LLM to classify content type\n",
        "    # Prompt should ask: Is this a comment, post, or general text?\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 3: Implement Moderation Check Nodes\n",
        "\n",
        "Create nodes for different types of moderation checks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1: Toxicity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_toxicity(state: ModerationState) -> ModerationState:\n",
        "    \"\"\"\n",
        "    Check for toxic/harmful content.\n",
        "    \n",
        "    Should detect:\n",
        "    - Hate speech\n",
        "    - Harassment\n",
        "    - Threats\n",
        "    - Profanity\n",
        "    \"\"\"\n",
        "    # TODO: Implement toxicity detection\n",
        "    # Use LLM to rate toxicity from 0.0 to 1.0\n",
        "    # If score > threshold, add to violations list\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2: Spam Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_spam(state: ModerationState) -> ModerationState:\n",
        "    \"\"\"\n",
        "    Check for spam content.\n",
        "    \n",
        "    Should detect:\n",
        "    - Promotional content\n",
        "    - Repetitive text\n",
        "    - Suspicious links\n",
        "    - Advertisement\n",
        "    \"\"\"\n",
        "    # TODO: Implement spam detection\n",
        "    # Use LLM to rate spam likelihood from 0.0 to 1.0\n",
        "    # If score > threshold, add to violations list\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3: PII Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_pii(state: ModerationState) -> ModerationState:\n",
        "    \"\"\"\n",
        "    Check for Personal Identifiable Information.\n",
        "    \n",
        "    Should detect:\n",
        "    - Email addresses\n",
        "    - Phone numbers\n",
        "    - Social Security Numbers\n",
        "    - Credit card numbers\n",
        "    - Physical addresses\n",
        "    \"\"\"\n",
        "    # TODO: Implement PII detection\n",
        "    # Use LLM or regex patterns\n",
        "    # If PII found, add to violations list\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 4: Implement Decision Node\n",
        "\n",
        "Make moderation decision based on check results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_decision(state: ModerationState) -> ModerationState:\n",
        "    \"\"\"\n",
        "    Make final moderation decision.\n",
        "    \n",
        "    Decision logic:\n",
        "    - REJECT: High toxicity (>0.8) OR multiple severe violations\n",
        "    - REVIEW: Medium toxicity (>0.5) OR any violations present\n",
        "    - APPROVE: No violations, low scores\n",
        "    \"\"\"\n",
        "    # TODO: Implement decision logic\n",
        "    # Consider all scores and violations\n",
        "    # Set decision and human_review_required fields\n",
        "    # Add timestamp\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 5: Implement Routing Logic\n",
        "\n",
        "Route content based on decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_decision(state: ModerationState) -> str:\n",
        "    \"\"\"\n",
        "    Route based on moderation decision.\n",
        "    \n",
        "    Returns:\n",
        "        - \"approve\": Content is safe\n",
        "        - \"review\": Needs human review\n",
        "        - \"reject\": Content is rejected\n",
        "    \"\"\"\n",
        "    # TODO: Implement routing logic\n",
        "    # Return one of: \"approve\", \"review\", \"reject\"\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 6: Implement Action Nodes\n",
        "\n",
        "Handle approved, reviewed, and rejected content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def approve_content(state: ModerationState) -> ModerationState:\n",
        "    \"\"\"\n",
        "    Handle approved content.\n",
        "    \"\"\"\n",
        "    # TODO: Log approval and return final state\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "def flag_for_review(state: ModerationState) -> ModerationState:\n",
        "    \"\"\"\n",
        "    Flag content for human review.\n",
        "    \"\"\"\n",
        "    # TODO: Log review flag and return final state\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "def reject_content(state: ModerationState) -> ModerationState:\n",
        "    \"\"\"\n",
        "    Reject content.\n",
        "    \"\"\"\n",
        "    # TODO: Log rejection and return final state\n",
        "    # YOUR CODE HERE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 7: Build the Graph\n",
        "\n",
        "Assemble all nodes into a complete moderation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create the moderation graph\n",
        "# 1. Initialize StateGraph\n",
        "# 2. Add all nodes\n",
        "# 3. Set entry point\n",
        "# 4. Add edges for parallel checks\n",
        "# 5. Add conditional edges for routing\n",
        "# 6. Compile the graph\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "moderation_graph = # YOUR CODE HERE\n",
        "\n",
        "print(\"‚úÖ Moderation graph created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 8: Create ModerationPipeline Class\n",
        "\n",
        "Wrap the graph in a reusable class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModerationPipeline:\n",
        "    \"\"\"\n",
        "    Content moderation pipeline.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the moderation pipeline.\"\"\"\n",
        "        # TODO: Store the compiled graph\n",
        "        self.graph = moderation_graph\n",
        "        \n",
        "        # Track statistics\n",
        "        self.stats = {\n",
        "            \"total\": 0,\n",
        "            \"approved\": 0,\n",
        "            \"rejected\": 0,\n",
        "            \"flagged\": 0\n",
        "        }\n",
        "    \n",
        "    def moderate(self, content: str) -> dict:\n",
        "        \"\"\"\n",
        "        Moderate a piece of content.\n",
        "        \n",
        "        Args:\n",
        "            content: The content to moderate\n",
        "            \n",
        "        Returns:\n",
        "            Moderation result dictionary\n",
        "        \"\"\"\n",
        "        # TODO: Invoke graph with initial state\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        # TODO: Update statistics\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def get_statistics(self) -> dict:\n",
        "        \"\"\"\n",
        "        Get moderation statistics.\n",
        "        \"\"\"\n",
        "        # TODO: Return statistics\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "    \n",
        "    # BONUS: Implement these methods\n",
        "    \n",
        "    def batch_moderate(self, contents: list[str]) -> list[dict]:\n",
        "        \"\"\"\n",
        "        Moderate multiple pieces of content.\n",
        "        \"\"\"\n",
        "        # TODO (BONUS): Batch moderation\n",
        "        pass\n",
        "    \n",
        "    def export_report(self) -> str:\n",
        "        \"\"\"\n",
        "        Export moderation report.\n",
        "        \"\"\"\n",
        "        # TODO (BONUS): Generate report\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Testing Your Implementation\n",
        "\n",
        "Run these tests to verify your moderation pipeline works correctly:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 1: Clean Content (Should Approve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Test 1: Clean Content\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline = ModerationPipeline()\n",
        "\n",
        "clean_content = \"I really enjoyed this product. The quality is excellent and shipping was fast!\"\n",
        "\n",
        "result = pipeline.moderate(clean_content)\n",
        "\n",
        "print(f\"Content: {clean_content}\")\n",
        "print(f\"\\nDecision: {result['decision']}\")\n",
        "print(f\"Toxicity Score: {result.get('toxicity_score', 0):.2f}\")\n",
        "print(f\"Spam Score: {result.get('spam_score', 0):.2f}\")\n",
        "print(f\"Violations: {result.get('violations', [])}\")\n",
        "\n",
        "# ‚úÖ Should be approved!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Toxic Content (Should Reject)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 2: Toxic Content\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "toxic_content = \"You're an idiot and I hate everything about you. This is terrible!\"\n",
        "\n",
        "result = pipeline.moderate(toxic_content)\n",
        "\n",
        "print(f\"Content: {toxic_content}\")\n",
        "print(f\"\\nDecision: {result['decision']}\")\n",
        "print(f\"Toxicity Score: {result.get('toxicity_score', 0):.2f}\")\n",
        "print(f\"Violations: {result.get('violations', [])}\")\n",
        "\n",
        "# ‚úÖ Should be rejected!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: Spam Content (Should Flag/Reject)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 3: Spam Content\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "spam_content = \"BUY NOW!!! Limited time offer!!! Click here: www.sketchy-deals.com Get rich quick!!!\"\n",
        "\n",
        "result = pipeline.moderate(spam_content)\n",
        "\n",
        "print(f\"Content: {spam_content}\")\n",
        "print(f\"\\nDecision: {result['decision']}\")\n",
        "print(f\"Spam Score: {result.get('spam_score', 0):.2f}\")\n",
        "print(f\"Violations: {result.get('violations', [])}\")\n",
        "\n",
        "# ‚úÖ Should be rejected or flagged!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 4: PII Content (Should Flag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 4: PII Content\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pii_content = \"My email is john.doe@email.com and my phone is 555-123-4567\"\n",
        "\n",
        "result = pipeline.moderate(pii_content)\n",
        "\n",
        "print(f\"Content: {pii_content}\")\n",
        "print(f\"\\nDecision: {result['decision']}\")\n",
        "print(f\"PII Detected: {result.get('pii_detected', False)}\")\n",
        "print(f\"Violations: {result.get('violations', [])}\")\n",
        "\n",
        "# ‚úÖ Should be flagged for review!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 5: Borderline Content (Should Flag for Review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 5: Borderline Content\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "borderline_content = \"This product is garbage and waste of money, but the customer service was okay.\"\n",
        "\n",
        "result = pipeline.moderate(borderline_content)\n",
        "\n",
        "print(f\"Content: {borderline_content}\")\n",
        "print(f\"\\nDecision: {result['decision']}\")\n",
        "print(f\"Toxicity Score: {result.get('toxicity_score', 0):.2f}\")\n",
        "print(f\"Human Review Required: {result.get('human_review_required', False)}\")\n",
        "\n",
        "# ‚úÖ Should be flagged for human review!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 6: Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTest 6: Statistics\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "stats = pipeline.get_statistics()\n",
        "\n",
        "print(\"üìä Moderation Statistics:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# ‚úÖ Should show accurate counts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üé® Your Own Tests\n",
        "\n",
        "Add your own test cases here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR TEST CASES HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Self-Assessment\n",
        "\n",
        "Rate your implementation (1-5):\n",
        "\n",
        "| Criteria | Rating | Notes |\n",
        "|----------|--------|-------|\n",
        "| State Management | /5 | TypedDict used correctly? |\n",
        "| Parallel Checks | /5 | Multiple checks run? |\n",
        "| Decision Logic | /5 | Correct routing? |\n",
        "| Accuracy | /5 | Detects violations? |\n",
        "| Edge Cases | /5 | Handles edge cases? |\n",
        "| Code Quality | /5 | Clean, documented? |\n",
        "| Bonus Features | /5 | Extra features? |\n",
        "| **Total** | **/35** | |\n",
        "\n",
        "---\n",
        "\n",
        "## ü§î Reflection Questions\n",
        "\n",
        "Answer these questions in the markdown cell below:\n",
        "\n",
        "1. How did parallel checks improve the moderation pipeline?\n",
        "2. What challenges did you face with conditional routing?\n",
        "3. How would you tune the threshold values for production?\n",
        "4. What additional checks would improve accuracy?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Answers:\n",
        "\n",
        "**1. Parallel Checks:**\n",
        "- [Your answer here]\n",
        "\n",
        "**2. Conditional Routing Challenges:**\n",
        "- [Your answer here]\n",
        "\n",
        "**3. Threshold Tuning:**\n",
        "- [Your answer here]\n",
        "\n",
        "**4. Additional Checks:**\n",
        "- [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì§ Submission\n",
        "\n",
        "### Before Submitting:\n",
        "\n",
        "- [ ] All tests pass\n",
        "- [ ] Parallel checks implemented\n",
        "- [ ] Conditional routing works\n",
        "- [ ] Accurate moderation decisions\n",
        "- [ ] Statistics tracking works\n",
        "- [ ] Code is well-documented\n",
        "- [ ] Reflection questions answered\n",
        "- [ ] Notebook runs from top to bottom\n",
        "\n",
        "### How to Submit:\n",
        "\n",
        "1. Save this notebook\n",
        "2. Commit: `git commit -m \"Complete Week 4 Exercise 1\"`\n",
        "3. Push: `git push origin week4-exercise1`\n",
        "4. Submit repository link\n",
        "\n",
        "---\n",
        "\n",
        "**Great work on your moderation pipeline! üéâ**"
      ]
    }
  ],
  "metadata": {
    "kernelnel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
