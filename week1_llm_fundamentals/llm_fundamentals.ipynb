{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1: LLM Fundamentals & Basic Chatbots\n",
        "\n",
        "## üìö Session Overview\n",
        "\n",
        "**Duration:** 2 hours  \n",
        "**Week:** 1  \n",
        "**Instructor-Led Session**\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this session, you will be able to:\n",
        "1. Understand what LLMs are and how they work\n",
        "2. Make API calls to OpenAI's GPT models\n",
        "3. Implement conversation history and memory\n",
        "4. Create streaming responses\n",
        "5. Apply basic prompt engineering techniques\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "- ‚úÖ Python 3.10+\n",
        "- ‚úÖ OpenAI API key\n",
        "\n",
        "## ‚è±Ô∏è Estimated Time\n",
        "\n",
        "- Setup & Introduction: 10 minutes\n",
        "- Section 1 (LLM Basics): 30 minutes\n",
        "- Section 2 (First Chatbot): 30 minutes\n",
        "- Section 3 (Memory & History): 25 minutes\n",
        "- Section 4 (Prompt Engineering): 20 minutes\n",
        "- Wrap-up & Q&A: 5 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 1: Introduction to LLMs\n",
        "\n",
        "## What are Large Language Models?\n",
        "\n",
        "Large Language Models (LLMs) are AI models trained on vast amounts of text data to:\n",
        "- **Understand** natural language\n",
        "- **Generate** human-like text\n",
        "- **Complete** tasks based on instructions\n",
        "\n",
        "### Popular LLMs:\n",
        "- **GPT-4** / GPT-3.5 (OpenAI)\n",
        "- **Claude** (Anthropic)\n",
        "- **Llama** (Meta)\n",
        "- **Gemini** (Google)\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### 1. **Tokens**\n",
        "- LLMs process text as tokens (roughly 4 characters per token)\n",
        "- \"Hello World\" ‚âà 2 tokens\n",
        "- Token limits vary by model (e.g., GPT-3.5: 4K, GPT-4: 8K-128K)\n",
        "\n",
        "### 2. **Temperature** (0.0 - 2.0)\n",
        "- Controls randomness/creativity\n",
        "- **Low (0.0-0.3)**: Deterministic, consistent\n",
        "- **Medium (0.5-0.7)**: Balanced\n",
        "- **High (0.8-2.0)**: Creative, varied\n",
        "\n",
        "### 3. **Context Window**\n",
        "- How much text the model can \"remember\" at once\n",
        "- Includes both input and output\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1: Your First API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple completion\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is an AI agent?\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "print(\"ü§ñ Response:\")\n",
        "print(response.choices[0].message.content)\n",
        "print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Understanding the Response Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine the response structure\n",
        "print(\"Response object structure:\")\n",
        "print(f\"ID: {response.id}\")\n",
        "print(f\"Model: {response.model}\")\n",
        "print(f\"Created: {datetime.fromtimestamp(response.created)}\")\n",
        "print(f\"\\nUsage:\")\n",
        "print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
        "print(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"  Total tokens: {response.usage.total_tokens}\")\n",
        "print(f\"\\nMessage role: {response.choices[0].message.role}\")\n",
        "print(f\"Finish reason: {response.choices[0].finish_reason}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2: Experimenting with Temperature\n",
        "\n",
        "Let's see how temperature affects responses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Write a creative tagline for an AI chatbot.\"\n",
        "\n",
        "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
        "\n",
        "for temp in temperatures:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temp,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    \n",
        "    print(f\"üå°Ô∏è Temperature {temp}:\")\n",
        "    print(f\"   {response.choices[0].message.content}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Try It Yourself!\n",
        "\n",
        "**Exercise:** Ask the LLM to explain a technical concept at different temperatures.\n",
        "\n",
        "Observe how:\n",
        "- Low temperature = consistent, factual\n",
        "- High temperature = creative, varied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Try different temperatures and compare outputs\n",
        "\n",
        "your_prompt = \"Explain what a neural network is in simple terms.\"\n",
        "\n",
        "# TODO: Test with temperature 0.0 and 1.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 2: Building Your First Chatbot\n",
        "\n",
        "Now let's build a simple chatbot with a conversation loop!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1: Simple Single-Turn Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_chatbot(user_message: str) -> str:\n",
        "    \"\"\"\n",
        "    A simple chatbot that responds to a single message.\n",
        "    No memory - each call is independent.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test it\n",
        "print(\"ü§ñ:\", simple_chatbot(\"Hello! What can you help me with?\"))\n",
        "print()\n",
        "print(\"ü§ñ:\", simple_chatbot(\"What did I just ask you?\"))  # It won't remember!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚ö†Ô∏è Problem: No Memory!\n",
        "\n",
        "The chatbot doesn't remember previous messages. Let's fix that!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2: Adding System Prompts\n",
        "\n",
        "System prompts define the chatbot's personality and behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chatbot_with_personality(user_message: str, personality: str = \"helpful\") -> str:\n",
        "    \"\"\"\n",
        "    Chatbot with different personalities based on system prompt.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Different system prompts for different personalities\n",
        "    system_prompts = {\n",
        "        \"helpful\": \"You are a helpful and friendly AI assistant.\",\n",
        "        \"professional\": \"You are a professional business consultant. Be formal and concise.\",\n",
        "        \"casual\": \"You are a casual, fun friend. Use emojis and keep it light!\",\n",
        "        \"teacher\": \"You are a patient teacher. Explain concepts clearly with examples.\"\n",
        "    }\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompts.get(personality, system_prompts[\"helpful\"])},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test different personalities\n",
        "question = \"How do I learn Python?\"\n",
        "\n",
        "print(\"üëî Professional:\")\n",
        "print(chatbot_with_personality(question, \"professional\"))\n",
        "print()\n",
        "\n",
        "print(\"üòÑ Casual:\")\n",
        "print(chatbot_with_personality(question, \"casual\"))\n",
        "print()\n",
        "\n",
        "print(\"üë®‚Äçüè´ Teacher:\")\n",
        "print(chatbot_with_personality(question, \"teacher\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Try It Yourself!\n",
        "\n",
        "**Exercise:** Create your own custom personality!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create a custom system prompt and test it\n",
        "\n",
        "custom_system_prompt = \"\"\"You are a pirate captain. \n",
        "Speak like a pirate and give advice about sailing the seven seas!\"\"\"\n",
        "\n",
        "# TODO: Use this system prompt to respond to a question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 3: Conversation History & Memory\n",
        "\n",
        "Let's build a chatbot that remembers the conversation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1: Chatbot with Full Conversation History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    A chatbot that maintains conversation history.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, system_prompt: str = \"You are a helpful AI assistant.\"):\n",
        "        self.messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt}\n",
        "        ]\n",
        "    \n",
        "    def chat(self, user_message: str) -> str:\n",
        "        \"\"\"\n",
        "        Send a message and get a response, maintaining history.\n",
        "        \"\"\"\n",
        "        # Add user message to history\n",
        "        self.messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message\n",
        "        })\n",
        "        \n",
        "        # Get response from API\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=self.messages,\n",
        "            temperature=0.7,\n",
        "            max_tokens=200\n",
        "        )\n",
        "        \n",
        "        # Extract assistant's response\n",
        "        assistant_message = response.choices[0].message.content\n",
        "        \n",
        "        # Add assistant's response to history\n",
        "        self.messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_message\n",
        "        })\n",
        "        \n",
        "        return assistant_message\n",
        "    \n",
        "    def get_history(self) -> List[Dict]:\n",
        "        \"\"\"Get the conversation history.\"\"\"\n",
        "        return self.messages\n",
        "    \n",
        "    def clear_history(self):\n",
        "        \"\"\"Clear conversation history (keep system prompt).\"\"\"\n",
        "        self.messages = [self.messages[0]]  # Keep only system prompt\n",
        "\n",
        "\n",
        "# Test the chatbot\n",
        "bot = Chatbot(system_prompt=\"You are a friendly AI assistant who loves to help!\")\n",
        "\n",
        "print(\"üë§: Hi, my name is Alex.\")\n",
        "print(f\"ü§ñ: {bot.chat('Hi, my name is Alex.')}\")\n",
        "print()\n",
        "\n",
        "print(\"üë§: What's my name?\")\n",
        "print(f\"ü§ñ: {bot.chat('What is my name?')}\")\n",
        "print()\n",
        "\n",
        "print(\"üë§: I love programming in Python.\")\n",
        "print(f\"ü§ñ: {bot.chat('I love programming in Python.')}\")\n",
        "print()\n",
        "\n",
        "print(\"üë§: What programming language did I mention?\")\n",
        "print(f\"ü§ñ: {bot.chat('What programming language did I mention?')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2: Viewing Conversation History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the conversation history\n",
        "print(\"üìú Conversation History:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, message in enumerate(bot.get_history(), 1):\n",
        "    role = message[\"role\"].upper()\n",
        "    content = message[\"content\"]\n",
        "    print(f\"\\n{i}. [{role}]\")\n",
        "    print(f\"   {content}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3: Managing Memory with Window Size\n",
        "\n",
        "Keeping all history can exceed token limits. Let's implement a sliding window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChatbotWithMemoryWindow:\n",
        "    \"\"\"\n",
        "    Chatbot that keeps only the last N messages to manage token limits.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, system_prompt: str = \"You are a helpful AI assistant.\", \n",
        "                 max_history: int = 10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            system_prompt: The system message\n",
        "            max_history: Maximum number of messages to keep (not counting system)\n",
        "        \"\"\"\n",
        "        self.system_prompt = {\"role\": \"system\", \"content\": system_prompt}\n",
        "        self.messages = []\n",
        "        self.max_history = max_history\n",
        "    \n",
        "    def chat(self, user_message: str) -> str:\n",
        "        \"\"\"Send a message and get a response.\"\"\"\n",
        "        # Add user message\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        \n",
        "        # Trim history if needed (keep last max_history messages)\n",
        "        if len(self.messages) > self.max_history:\n",
        "            self.messages = self.messages[-self.max_history:]\n",
        "        \n",
        "        # Build messages for API (system + history)\n",
        "        api_messages = [self.system_prompt] + self.messages\n",
        "        \n",
        "        # Get response\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=api_messages,\n",
        "            temperature=0.7,\n",
        "            max_tokens=200\n",
        "        )\n",
        "        \n",
        "        assistant_message = response.choices[0].message.content\n",
        "        \n",
        "        # Add assistant response\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "        \n",
        "        # Trim again if needed\n",
        "        if len(self.messages) > self.max_history:\n",
        "            self.messages = self.messages[-self.max_history:]\n",
        "        \n",
        "        return assistant_message\n",
        "\n",
        "\n",
        "# Test with limited history\n",
        "limited_bot = ChatbotWithMemoryWindow(max_history=6)  # Keep last 3 exchanges\n",
        "\n",
        "print(f\"ü§ñ: {limited_bot.chat('Hi, I am learning about AI.')}\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('My favorite color is blue.')}\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('I have a dog named Max.')}\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('I work as a software engineer.')}\")\n",
        "print()\n",
        "print(\"Now testing memory...\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('What am I learning about?')}\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('What is my favorite color?')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Try It Yourself!\n",
        "\n",
        "**Exercise:** Test the memory window by having a long conversation and seeing what gets forgotten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create a chatbot with max_history=4 and test memory limits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 4: Streaming Responses\n",
        "\n",
        "Streaming makes chatbots feel more responsive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1: Basic Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_response(user_message: str):\n",
        "    \"\"\"\n",
        "    Stream the response token by token.\n",
        "    \"\"\"\n",
        "    print(\"ü§ñ: \", end=\"\", flush=True)\n",
        "    \n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=200,\n",
        "        stream=True  # Enable streaming!\n",
        "    )\n",
        "    \n",
        "    full_response = \"\"\n",
        "    \n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            content = chunk.choices[0].delta.content\n",
        "            print(content, end=\"\", flush=True)\n",
        "            full_response += content\n",
        "    \n",
        "    print()  # New line after streaming\n",
        "    return full_response\n",
        "\n",
        "# Test streaming\n",
        "response = stream_response(\"Tell me a short story about a robot learning to code.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2: Chatbot with Streaming Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StreamingChatbot:\n",
        "    \"\"\"\n",
        "    Chatbot with streaming support and conversation history.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, system_prompt: str = \"You are a helpful AI assistant.\"):\n",
        "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "    \n",
        "    def chat(self, user_message: str, stream: bool = False) -> str:\n",
        "        \"\"\"\n",
        "        Chat with optional streaming.\n",
        "        \n",
        "        Args:\n",
        "            user_message: The user's message\n",
        "            stream: Whether to stream the response\n",
        "        \"\"\"\n",
        "        # Add user message\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        \n",
        "        if stream:\n",
        "            # Streaming response\n",
        "            print(\"ü§ñ: \", end=\"\", flush=True)\n",
        "            \n",
        "            response_stream = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=200,\n",
        "                stream=True\n",
        "            )\n",
        "            \n",
        "            full_response = \"\"\n",
        "            for chunk in response_stream:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    content = chunk.choices[0].delta.content\n",
        "                    print(content, end=\"\", flush=True)\n",
        "                    full_response += content\n",
        "            \n",
        "            print()  # New line\n",
        "            \n",
        "        else:\n",
        "            # Non-streaming response\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=200\n",
        "            )\n",
        "            full_response = response.choices[0].message.content\n",
        "        \n",
        "        # Add assistant response to history\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "        \n",
        "        return full_response\n",
        "\n",
        "\n",
        "# Test streaming chatbot\n",
        "streaming_bot = StreamingChatbot()\n",
        "\n",
        "print(\"Testing with streaming:\")\n",
        "streaming_bot.chat(\"Explain what machine learning is.\", stream=True)\n",
        "print()\n",
        "\n",
        "print(\"\\nTesting without streaming:\")\n",
        "response = streaming_bot.chat(\"What are the main types of machine learning?\", stream=False)\n",
        "print(f\"ü§ñ: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 5: Prompt Engineering Basics\n",
        "\n",
        "Learn how to write better prompts for better responses!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1: Be Specific and Clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ùå Vague prompt\n",
        "vague_prompt = \"Tell me about Python.\"\n",
        "\n",
        "# ‚úÖ Specific prompt\n",
        "specific_prompt = \"\"\"Explain Python programming language to a beginner who has never coded before. \n",
        "Include: what it's used for, why it's popular, and one simple example.\n",
        "Keep the explanation under 100 words.\"\"\"\n",
        "\n",
        "print(\"Vague Prompt Response:\")\n",
        "print(simple_chatbot(vague_prompt))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"Specific Prompt Response:\")\n",
        "print(simple_chatbot(specific_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2: Few-Shot Learning (Provide Examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using examples to teach the model a pattern\n",
        "few_shot_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You convert sentences into emoji summaries.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love coding in Python.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"‚ù§Ô∏èüíªüêç\"},\n",
        "    {\"role\": \"user\", \"content\": \"I went to the beach and saw dolphins.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"üèñÔ∏èüê¨\"},\n",
        "    {\"role\": \"user\", \"content\": \"I ate pizza and watched a movie.\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=few_shot_messages,\n",
        "    temperature=0.3,\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(\"ü§ñ:\", response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3: Prompt Templates\n",
        "\n",
        "Reusable templates for common tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt_from_template(template: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    Create a prompt from a template with variables.\n",
        "    \"\"\"\n",
        "    return template.format(**kwargs)\n",
        "\n",
        "# Define templates\n",
        "SUMMARIZE_TEMPLATE = \"\"\"\n",
        "Summarize the following text in {num_sentences} sentences:\n",
        "\n",
        "{text}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "TRANSLATE_TEMPLATE = \"\"\"\n",
        "Translate the following text from {source_lang} to {target_lang}:\n",
        "\n",
        "{text}\n",
        "\n",
        "Translation:\n",
        "\"\"\"\n",
        "\n",
        "# Use the template\n",
        "text_to_summarize = \"\"\"\n",
        "Artificial Intelligence (AI) is intelligence demonstrated by machines, \n",
        "in contrast to the natural intelligence displayed by humans and animals. \n",
        "Leading AI textbooks define the field as the study of \"intelligent agents\": \n",
        "any device that perceives its environment and takes actions that maximize \n",
        "its chance of successfully achieving its goals.\n",
        "\"\"\"\n",
        "\n",
        "prompt = create_prompt_from_template(\n",
        "    SUMMARIZE_TEMPLATE,\n",
        "    num_sentences=2,\n",
        "    text=text_to_summarize\n",
        ")\n",
        "\n",
        "print(\"Generated Prompt:\")\n",
        "print(prompt)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "response = simple_chatbot(prompt)\n",
        "print(\"ü§ñ Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4: Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Without chain of thought\n",
        "simple_math = \"What is 15% of 240?\"\n",
        "\n",
        "# With chain of thought\n",
        "cot_math = \"\"\"\n",
        "What is 15% of 240?\n",
        "\n",
        "Let's solve this step by step:\n",
        "1. First, understand what we need to find\n",
        "2. Convert the percentage to decimal\n",
        "3. Multiply\n",
        "4. Give the final answer\n",
        "\"\"\"\n",
        "\n",
        "print(\"Simple prompt:\")\n",
        "print(simple_chatbot(simple_math))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"Chain of thought prompt:\")\n",
        "print(simple_chatbot(cot_math))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Try It Yourself!\n",
        "\n",
        "**Exercise:** Create a prompt template for code explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create a template that explains code in simple terms\n",
        "\n",
        "CODE_EXPLAIN_TEMPLATE = \"\"\"\n",
        "# TODO: Create your template here\n",
        "# Variables to include: {code}, {language}, {skill_level}\n",
        "\"\"\"\n",
        "\n",
        "# Test it with a code snippet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéØ Summary & Key Takeaways\n",
        "\n",
        "## What We Learned:\n",
        "\n",
        "### 1. **LLM Fundamentals**\n",
        "- Understanding tokens, temperature, and context windows\n",
        "- Making API calls to OpenAI\n",
        "- Examining response objects\n",
        "\n",
        "### 2. **Building Chatbots**\n",
        "- Single-turn vs. multi-turn conversations\n",
        "- System prompts for personality\n",
        "- Conversation history management\n",
        "\n",
        "### 3. **Memory Management**\n",
        "- Maintaining conversation context\n",
        "- Sliding window approach for token limits\n",
        "- Trade-offs between memory and performance\n",
        "\n",
        "### 4. **Streaming Responses**\n",
        "- Creating responsive, real-time chatbots\n",
        "- Handling streamed chunks\n",
        "\n",
        "### 5. **Prompt Engineering**\n",
        "- Writing clear, specific prompts\n",
        "- Few-shot learning with examples\n",
        "- Prompt templates for reusability\n",
        "- Chain of thought reasoning\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Next Steps:\n",
        "\n",
        "### Exercises for This Week:\n",
        "\n",
        "**Exercise 1 (Due Monday):** `02_exercise_personal_assistant.ipynb`\n",
        "- Build a personal assistant chatbot\n",
        "- Implement memory and personalities\n",
        "- Add prompt templates\n",
        "\n",
        "**Exercise 2 (Due Friday):** `03_exercise_domain_chatbot.ipynb`\n",
        "- Create a domain-specific chatbot\n",
        "- Implement streaming\n",
        "- Add input validation\n",
        "\n",
        "---\n",
        "\n",
        "## ü§î Reflection Questions:\n",
        "\n",
        "1. When would you use high vs. low temperature?\n",
        "2. Why is conversation history important?\n",
        "3. What are the trade-offs of keeping all conversation history?\n",
        "4. How does prompt engineering improve responses?\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Additional Resources:\n",
        "\n",
        "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
        "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
        "- [Best Practices for Prompt Engineering](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùì Questions?\n",
        "\n",
        "**Office Hours:** Monday & Friday check-ins  \n",
        "**Next Session:** Week 2 - LangChain Core Concepts\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Coding! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
