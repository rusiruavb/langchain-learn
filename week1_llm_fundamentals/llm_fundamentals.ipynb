{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1: LLM Fundamentals & Basic Chatbots\n",
        "\n",
        "## üìö Session Overview\n",
        "\n",
        "**Duration:** 2 hours  \n",
        "**Week:** 1  \n",
        "**Instructor-Led Session**\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this session, you will be able to:\n",
        "1. Understand what LLMs are and how they work\n",
        "2. Make API calls to OpenAI's GPT models\n",
        "3. Implement conversation history and memory\n",
        "4. Create streaming responses\n",
        "5. Apply basic prompt engineering techniques\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "- ‚úÖ Python 3.10+\n",
        "- ‚úÖ OpenAI API key\n",
        "\n",
        "## ‚è±Ô∏è Estimated Time\n",
        "\n",
        "- Setup & Introduction: 10 minutes\n",
        "- Section 1 (LLM Basics): 30 minutes\n",
        "- Section 2 (First Chatbot): 30 minutes\n",
        "- Section 3 (Memory & History): 25 minutes\n",
        "- Section 4 (Prompt Engineering): 20 minutes\n",
        "- Wrap-up & Q&A: 5 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Setup complete!\n",
            "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\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 1: Introduction to LLMs\n",
        "\n",
        "## What are Large Language Models?\n",
        "\n",
        "Large Language Models (LLMs) are AI models trained on vast amounts of text data to:\n",
        "- **Understand** natural language\n",
        "- **Generate** human-like text\n",
        "- **Complete** tasks based on instructions\n",
        "\n",
        "### Popular LLMs:\n",
        "- **GPT-4** / GPT-3.5 (OpenAI)\n",
        "- **Claude** (Anthropic)\n",
        "- **Llama** (Meta)\n",
        "- **Gemini** (Google)\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### 1. **Tokens**\n",
        "- LLMs process text as tokens (roughly 4 characters per token)\n",
        "- \"Hello World\" ‚âà 2 tokens\n",
        "- Token limits vary by model (e.g., GPT-3.5: 4K, GPT-4: 8K-128K)\n",
        "\n",
        "### 2. **Temperature** (0.0 - 2.0)\n",
        "- Controls randomness/creativity\n",
        "- **Low (0.0-0.3)**: Deterministic, consistent\n",
        "- **Medium (0.5-0.7)**: Balanced\n",
        "- **High (0.8-2.0)**: Creative, varied\n",
        "\n",
        "### 3. **Context Window**\n",
        "- How much text the model can \"remember\" at once\n",
        "- Includes both input and output\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1: Your First API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Response:\n",
            "An AI agent is a software program that is designed to act on behalf of a user or autonomously in performing tasks or making decisions. It utilizes artificial intelligence techniques such as machine learning, natural language processing, and computer vision to understand and respond to its environment. AI agents can be found in a wide range of applications, such as virtual assistants, chatbots, autonomous vehicles, and automated trading systems.\n",
            "\n",
            "üìä Tokens used: 93\n"
          ]
        }
      ],
      "source": [
        "# Simple completion\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is an AI agent?\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "print(\"ü§ñ Response:\")\n",
        "print(response.choices[0].message.content)\n",
        "print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Understanding the Response Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response object structure:\n",
            "ID: chatcmpl-Co4tFTtN0tHDo3hHbcrQdxorInf9m\n",
            "Model: gpt-3.5-turbo-0125\n",
            "Created: 2025-12-18 15:17:13\n",
            "\n",
            "Usage:\n",
            "  Prompt tokens: 13\n",
            "  Completion tokens: 80\n",
            "  Total tokens: 93\n",
            "\n",
            "Message role: assistant\n",
            "Finish reason: stop\n"
          ]
        }
      ],
      "source": [
        "# Let's examine the response structure\n",
        "print(\"Response object structure:\")\n",
        "print(f\"ID: {response.id}\")\n",
        "print(f\"Model: {response.model}\")\n",
        "print(f\"Created: {datetime.fromtimestamp(response.created)}\")\n",
        "print(f\"\\nUsage:\")\n",
        "print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
        "print(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"  Total tokens: {response.usage.total_tokens}\")\n",
        "print(f\"\\nMessage role: {response.choices[0].message.role}\")\n",
        "print(f\"Finish reason: {response.choices[0].finish_reason}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2: Experimenting with Temperature\n",
        "\n",
        "Let's see how temperature affects responses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Write a creative tagline for an AI chatbot.\"\n",
        "\n",
        "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
        "\n",
        "for temp in temperatures:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temp,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    \n",
        "    print(f\"üå°Ô∏è Temperature {temp}:\")\n",
        "    print(f\"   {response.choices[0].message.content}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Try It Yourself!\n",
        "\n",
        "**Exercise:** Ask the LLM to explain a technical concept at different temperatures.\n",
        "\n",
        "Observe how:\n",
        "- Low temperature = consistent, factual\n",
        "- High temperature = creative, varied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Try different temperatures and compare outputs\n",
        "\n",
        "your_prompt = \"Explain what a neural network is in simple terms.\"\n",
        "\n",
        "# TODO: Test with temperature 0.0 and 1.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 2: Building Your First Chatbot\n",
        "\n",
        "Now let's build a simple chatbot with a conversation loop!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1: Simple Single-Turn Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ: Hello! I can help you with a variety of things such as answering questions, providing information, offering advice, or just having a friendly chat. Just let me know how I can assist you today!\n",
            "\n",
            "ü§ñ: You asked me: \"What did I just ask you?\"\n"
          ]
        }
      ],
      "source": [
        "def simple_chatbot(user_message: str) -> str:\n",
        "    \"\"\"\n",
        "    A simple chatbot that responds to a single message.\n",
        "    No memory - each call is independent.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test it\n",
        "print(\"ü§ñ:\", simple_chatbot(\"Hello! What can you help me with?\"))\n",
        "print()\n",
        "print(\"ü§ñ:\", simple_chatbot(\"What did I just ask you?\"))  # It won't remember!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚ö†Ô∏è Problem: No Memory!\n",
        "\n",
        "The chatbot doesn't remember previous messages. Let's fix that!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2: Adding System Prompts\n",
        "\n",
        "System prompts define the chatbot's personality and behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üëî Professional:\n",
            "To learn Python, I recommend the following steps:\n",
            "\n",
            "1. Start with the basics: Familiarize yourself with Python syntax, data types, functions, and control flow structures. Online tutorials, books, and courses can be helpful resources.\n",
            "\n",
            "2. Practice coding: The best way to learn Python is by practicing coding. Write small programs, solve coding challenges, and work on projects to reinforce your understanding.\n",
            "\n",
            "3. Explore Python libraries: Python has a rich ecosystem of libraries for various purposes such as data analysis, web development, and machine learning. Familiarize yourself with popular libraries like NumPy, Pandas, and Django.\n",
            "\n",
            "4. Join a community: Engage with the Python community through online forums, meetups, and conferences. Networking with other Python developers can help you learn new techniques and stay updated on best practices.\n",
            "\n",
            "5. Build projects: Apply your Python skills to real-world projects. Building projects not only enhances your coding skills but also showcases your abilities to potential employers.\n",
            "\n",
            "Remember that learning Python\n",
            "\n",
            "üòÑ Casual:\n",
            "Hey there! üêç Learning Python can be a blast! There are tons of online tutorials, YouTube videos, and interactive coding platforms like Codecademy or Udemy. You got this! üí™ Just dive in and start coding! üöÄ\n",
            "\n",
            "üë®‚Äçüè´ Teacher:\n",
            "Learning Python can be a rewarding experience as it is a versatile language used in a wide range of applications such as web development, data analysis, artificial intelligence, and automation. Here are some steps to help you get started with learning Python:\n",
            "\n",
            "1. **Set Clear Goals**: Determine your reason for learning Python. Whether you want to build web applications, analyze data, automate tasks, or pursue a career in programming, having a clear goal will guide your learning path.\n",
            "\n",
            "2. **Understand the Basics**: Start by familiarizing yourself with the basic concepts of Python such as variables, data types, control structures (if statements, loops), functions, and classes. Online tutorials, books, and courses can be helpful resources for learning the fundamentals.\n",
            "\n",
            "3. **Practice Regularly**: The best way to learn Python is by practicing regularly. Write code, experiment with different concepts, and work on small projects to reinforce your understanding.\n",
            "\n",
            "4. **Utilize Online Resources**: There are a plethora of free and paid online\n"
          ]
        }
      ],
      "source": [
        "def chatbot_with_personality(user_message: str, personality: str = \"helpful\") -> str:\n",
        "    \"\"\"\n",
        "    Chatbot with different personalities based on system prompt.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Different system prompts for different personalities\n",
        "    system_prompts = {\n",
        "        \"helpful\": \"You are a helpful and friendly AI assistant.\",\n",
        "        \"professional\": \"You are a professional business consultant. Be formal and concise.\",\n",
        "        \"casual\": \"You are a casual, fun friend. Use emojis and keep it light!\",\n",
        "        \"teacher\": \"You are a patient teacher. Explain concepts clearly with examples.\"\n",
        "    }\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompts.get(personality, system_prompts[\"helpful\"])},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test different personalities\n",
        "question = \"How do I learn Python?\"\n",
        "\n",
        "print(\"üëî Professional:\")\n",
        "print(chatbot_with_personality(question, \"professional\"))\n",
        "print()\n",
        "\n",
        "print(\"üòÑ Casual:\")\n",
        "print(chatbot_with_personality(question, \"casual\"))\n",
        "print()\n",
        "\n",
        "print(\"üë®‚Äçüè´ Teacher:\")\n",
        "print(chatbot_with_personality(question, \"teacher\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Try It Yourself!\n",
        "\n",
        "**Exercise:** Create your own custom personality!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create a custom system prompt and test it\n",
        "\n",
        "custom_system_prompt = \"\"\"You are a pirate captain. \n",
        "Speak like a pirate and give advice about sailing the seven seas!\"\"\"\n",
        "\n",
        "# TODO: Use this system prompt to respond to a question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 3: Conversation History & Memory\n",
        "\n",
        "Let's build a chatbot that remembers the conversation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1: Chatbot with Full Conversation History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üë§: Hi, my name is Alex.\n",
            "ü§ñ: Hello Alex! It's nice to meet you. How can I assist you today?\n",
            "\n",
            "üë§: What's my name?\n",
            "ü§ñ: Your name is Alex! If there's anything specific you'd like to know or do, feel free to ask.\n",
            "\n",
            "üë§: I love programming in Python.\n",
            "ü§ñ: That's wonderful to hear, Alex! Python is a versatile and powerful programming language. If you ever need help with Python programming or have any questions related to it, feel free to ask. I'm here to assist you!\n",
            "\n",
            "üë§: What programming language did I mention?\n",
            "ü§ñ: You mentioned that you love programming in Python! It's a great choice for various applications due to its readability and versatility. If you have any Python-related questions or need assistance with anything else, feel free to let me know.\n"
          ]
        }
      ],
      "source": [
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    A chatbot that maintains conversation history.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, system_prompt: str = \"You are a helpful AI assistant.\"):\n",
        "        self.messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt}\n",
        "        ]\n",
        "    \n",
        "    def chat(self, user_message: str) -> str:\n",
        "        \"\"\"\n",
        "        Send a message and get a response, maintaining history.\n",
        "        \"\"\"\n",
        "        # Add user message to history\n",
        "        self.messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message\n",
        "        })\n",
        "        \n",
        "        # Get response from API\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=self.messages,\n",
        "            temperature=0.7,\n",
        "            max_tokens=200\n",
        "        )\n",
        "        \n",
        "        # Extract assistant's response\n",
        "        assistant_message = response.choices[0].message.content\n",
        "        \n",
        "        # Add assistant's response to history\n",
        "        self.messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_message\n",
        "        })\n",
        "        \n",
        "        return assistant_message\n",
        "    \n",
        "    def get_history(self) -> List[Dict]:\n",
        "        \"\"\"Get the conversation history.\"\"\"\n",
        "        return self.messages\n",
        "    \n",
        "    def clear_history(self):\n",
        "        \"\"\"Clear conversation history (keep system prompt).\"\"\"\n",
        "        self.messages = [self.messages[0]]  # Keep only system prompt\n",
        "\n",
        "\n",
        "# Test the chatbot\n",
        "bot = Chatbot(system_prompt=\"You are a friendly AI assistant who loves to help!\")\n",
        "\n",
        "print(\"üë§: Hi, my name is Alex.\")\n",
        "print(f\"ü§ñ: {bot.chat('Hi, my name is Alex.')}\")\n",
        "print()\n",
        "\n",
        "print(\"üë§: What's my name?\")\n",
        "print(f\"ü§ñ: {bot.chat('What is my name?')}\")\n",
        "print()\n",
        "\n",
        "print(\"üë§: I love programming in Python.\")\n",
        "print(f\"ü§ñ: {bot.chat('I love programming in Python.')}\")\n",
        "print()\n",
        "\n",
        "print(\"üë§: What programming language did I mention?\")\n",
        "print(f\"ü§ñ: {bot.chat('What programming language did I mention?')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2: Viewing Conversation History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the conversation history\n",
        "print(\"üìú Conversation History:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, message in enumerate(bot.get_history(), 1):\n",
        "    role = message[\"role\"].upper()\n",
        "    content = message[\"content\"]\n",
        "    print(f\"\\n{i}. [{role}]\")\n",
        "    print(f\"   {content}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3: Managing Memory with Window Size\n",
        "\n",
        "Keeping all history can exceed token limits. Let's implement a sliding window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ: That's great to hear! AI is a fascinating and rapidly evolving field with many exciting applications across various industries. If you have any specific questions or topics you'd like to learn more about, feel free to ask!\n",
            "ü§ñ: That's a lovely choice! Blue is often associated with calmness, serenity, and stability. Is there anything else you'd like to share or discuss about AI or any other topic?\n",
            "ü§ñ: Max sounds like a wonderful companion! Dogs bring so much joy and love into our lives. If you have any questions about taking care of Max or if you'd like to share more about him, feel free to let me know!\n",
            "ü§ñ: That's fantastic! As a software engineer, you play a crucial role in developing innovative technologies and solutions that shape our modern world. If you ever need assistance with coding, programming languages, or any other software-related topics, feel free to reach out. I'm here to help!\n",
            "\n",
            "Now testing memory...\n",
            "ü§ñ: As a software engineer, you are likely learning about various programming languages, software development methodologies, data structures, algorithms, and new technologies in the field. Continuous learning and keeping up with industry trends are essential in the fast-paced world of technology. If you have any specific topics or questions in mind that you'd like to explore further, feel free to let me know!\n",
            "ü§ñ: As an AI assistant, I don't have access to personal information about you unless you explicitly share it during our conversation. If you'd like to share your favorite color with me, I'd be happy to know more about your preferences and interests. Feel free to share any other details or questions you have in mind!\n"
          ]
        }
      ],
      "source": [
        "class ChatbotWithMemoryWindow:\n",
        "    \"\"\"\n",
        "    Chatbot that keeps only the last N messages to manage token limits.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, system_prompt: str = \"You are a helpful AI assistant.\", \n",
        "                 max_history: int = 10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            system_prompt: The system message\n",
        "            max_history: Maximum number of messages to keep (not counting system)\n",
        "        \"\"\"\n",
        "        self.system_prompt = {\"role\": \"system\", \"content\": system_prompt}\n",
        "        self.messages = []\n",
        "        self.max_history = max_history\n",
        "    \n",
        "    def chat(self, user_message: str) -> str:\n",
        "        \"\"\"Send a message and get a response.\"\"\"\n",
        "        # Add user message\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        \n",
        "        # Trim history if needed (keep last max_history messages)\n",
        "        if len(self.messages) > self.max_history:\n",
        "            self.messages = self.messages[-self.max_history:]\n",
        "        \n",
        "        # Build messages for API (system + history)\n",
        "        api_messages = [self.system_prompt] + self.messages\n",
        "        \n",
        "        # Get response\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=api_messages,\n",
        "            temperature=0.7,\n",
        "            max_tokens=200\n",
        "        )\n",
        "        \n",
        "        assistant_message = response.choices[0].message.content\n",
        "        \n",
        "        # Add assistant response\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "        \n",
        "        # Trim again if needed\n",
        "        if len(self.messages) > self.max_history:\n",
        "            self.messages = self.messages[-self.max_history:]\n",
        "        \n",
        "        return assistant_message\n",
        "\n",
        "\n",
        "# Test with limited history\n",
        "limited_bot = ChatbotWithMemoryWindow(max_history=6)  # Keep last 3 exchanges\n",
        "\n",
        "print(f\"ü§ñ: {limited_bot.chat('Hi, I am learning about AI.')}\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('My favorite color is blue.')}\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('I have a dog named Max.')}\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('I work as a software engineer.')}\")\n",
        "print()\n",
        "print(\"Now testing memory...\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('What am I learning about?')}\")\n",
        "print(f\"ü§ñ: {limited_bot.chat('What is my favorite color?')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Try It Yourself!\n",
        "\n",
        "**Exercise:** Test the memory window by having a long conversation and seeing what gets forgotten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create a chatbot with max_history=4 and test memory limits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 4: Streaming Responses\n",
        "\n",
        "Streaming makes chatbots feel more responsive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1: Basic Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_response(user_message: str):\n",
        "    \"\"\"\n",
        "    Stream the response token by token.\n",
        "    \"\"\"\n",
        "    print(\"ü§ñ: \", end=\"\", flush=True)\n",
        "    \n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=200,\n",
        "        stream=True  # Enable streaming!\n",
        "    )\n",
        "    \n",
        "    full_response = \"\"\n",
        "    \n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            content = chunk.choices[0].delta.content\n",
        "            print(content, end=\"\", flush=True)\n",
        "            full_response += content\n",
        "    \n",
        "    print()  # New line after streaming\n",
        "    return full_response\n",
        "\n",
        "# Test streaming\n",
        "response = stream_response(\"Tell me a short story about a robot learning to code.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2: Chatbot with Streaming Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StreamingChatbot:\n",
        "    \"\"\"\n",
        "    Chatbot with streaming support and conversation history.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, system_prompt: str = \"You are a helpful AI assistant.\"):\n",
        "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "    \n",
        "    def chat(self, user_message: str, stream: bool = False) -> str:\n",
        "        \"\"\"\n",
        "        Chat with optional streaming.\n",
        "        \n",
        "        Args:\n",
        "            user_message: The user's message\n",
        "            stream: Whether to stream the response\n",
        "        \"\"\"\n",
        "        # Add user message\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        \n",
        "        if stream:\n",
        "            # Streaming response\n",
        "            print(\"ü§ñ: \", end=\"\", flush=True)\n",
        "            \n",
        "            response_stream = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=200,\n",
        "                stream=True\n",
        "            )\n",
        "            \n",
        "            full_response = \"\"\n",
        "            for chunk in response_stream:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    content = chunk.choices[0].delta.content\n",
        "                    print(content, end=\"\", flush=True)\n",
        "                    full_response += content\n",
        "            \n",
        "            print()  # New line\n",
        "            \n",
        "        else:\n",
        "            # Non-streaming response\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=200\n",
        "            )\n",
        "            full_response = response.choices[0].message.content\n",
        "        \n",
        "        # Add assistant response to history\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "        \n",
        "        return full_response\n",
        "\n",
        "\n",
        "# Test streaming chatbot\n",
        "streaming_bot = StreamingChatbot()\n",
        "\n",
        "print(\"Testing with streaming:\")\n",
        "streaming_bot.chat(\"Explain what machine learning is.\", stream=True)\n",
        "print()\n",
        "\n",
        "print(\"\\nTesting without streaming:\")\n",
        "response = streaming_bot.chat(\"What are the main types of machine learning?\", stream=False)\n",
        "print(f\"ü§ñ: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 5: Prompt Engineering Basics\n",
        "\n",
        "Learn how to write better prompts for better responses!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1: Be Specific and Clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vague Prompt Response:\n",
            "Python is a versatile and powerful programming language that is known for its simplicity and readability. It was created by Guido van Rossum and released in 1991. Python is widely used for web development, data analysis, artificial intelligence, scientific computing, and more. \n",
            "\n",
            "Some key features of Python include:\n",
            "\n",
            "1. Easy to learn and use: Python has a simple and intuitive syntax that makes it easy for beginners to pick up and understand.\n",
            "\n",
            "2. Interpreted language: Python code is executed line by line by the Python interpreter, making it easy to test and debug code.\n",
            "\n",
            "3. Extensive standard library: Python comes with a large standard library that provides support for a wide range of tasks, from working with files to networking.\n",
            "\n",
            "4. Community support: Python has a large and active community of developers who contribute to its ongoing development, provide support, and create libraries and frameworks.\n",
            "\n",
            "5. Object-oriented programming: Python supports object-oriented programming, allowing developers to create reusable and modular code.\n",
            "\n",
            "Overall,\n",
            "\n",
            "==================================================\n",
            "\n",
            "Specific Prompt Response:\n",
            "Python is a versatile programming language used for web development, data analysis, artificial intelligence, and more. Its popularity stems from its readability and simplicity, making it easy for beginners to learn. \n",
            "Example: \n",
            "```python\n",
            "print(\"Hello, World!\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# ‚ùå Vague prompt\n",
        "vague_prompt = \"Tell me about Python.\"\n",
        "\n",
        "# ‚úÖ Specific prompt\n",
        "specific_prompt = \"\"\"Explain Python programming language to a beginner who has never coded before. \n",
        "Include: what it's used for, why it's popular, and one simple example.\n",
        "Keep the explanation under 100 words.\"\"\"\n",
        "\n",
        "print(\"Vague Prompt Response:\")\n",
        "print(simple_chatbot(vague_prompt))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"Specific Prompt Response:\")\n",
        "print(simple_chatbot(specific_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2: Few-Shot Learning (Provide Examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ: üçïüé¨\n"
          ]
        }
      ],
      "source": [
        "# Using examples to teach the model a pattern\n",
        "few_shot_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You convert sentences into emoji summaries.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love coding in Python.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"‚ù§Ô∏èüíªüêç\"},\n",
        "    {\"role\": \"user\", \"content\": \"I went to the beach and saw dolphins.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"üèñÔ∏èüê¨\"},\n",
        "    {\"role\": \"user\", \"content\": \"I ate pizza and watched a movie.\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=few_shot_messages,\n",
        "    temperature=0.3,\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(\"ü§ñ:\", response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3: Prompt Templates\n",
        "\n",
        "Reusable templates for common tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Prompt:\n",
            "\n",
            "Summarize the following text in 2 sentences:\n",
            "\n",
            "\n",
            "Artificial Intelligence (AI) is intelligence demonstrated by machines, \n",
            "in contrast to the natural intelligence displayed by humans and animals. \n",
            "Leading AI textbooks define the field as the study of \"intelligent agents\": \n",
            "any device that perceives its environment and takes actions that maximize \n",
            "its chance of successfully achieving its goals.\n",
            "\n",
            "\n",
            "Summary:\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "ü§ñ Response:\n",
            "Artificial Intelligence is the intelligence exhibited by machines, as opposed to the natural intelligence of humans and animals. AI is defined as the study of \"intelligent agents\" - devices that perceive their environment and take actions to achieve their goals.\n"
          ]
        }
      ],
      "source": [
        "def create_prompt_from_template(template: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    Create a prompt from a template with variables.\n",
        "    \"\"\"\n",
        "    return template.format(**kwargs)\n",
        "\n",
        "# Define templates\n",
        "SUMMARIZE_TEMPLATE = \"\"\"\n",
        "Summarize the following text in {num_sentences} sentences:\n",
        "\n",
        "{text}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "TRANSLATE_TEMPLATE = \"\"\"\n",
        "Translate the following text from {source_lang} to {target_lang}:\n",
        "\n",
        "{text}\n",
        "\n",
        "Translation:\n",
        "\"\"\"\n",
        "\n",
        "# Use the template\n",
        "text_to_summarize = \"\"\"\n",
        "Artificial Intelligence (AI) is intelligence demonstrated by machines, \n",
        "in contrast to the natural intelligence displayed by humans and animals. \n",
        "Leading AI textbooks define the field as the study of \"intelligent agents\": \n",
        "any device that perceives its environment and takes actions that maximize \n",
        "its chance of successfully achieving its goals.\n",
        "\"\"\"\n",
        "\n",
        "prompt = create_prompt_from_template(\n",
        "    SUMMARIZE_TEMPLATE,\n",
        "    num_sentences=2,\n",
        "    text=text_to_summarize\n",
        ")\n",
        "\n",
        "print(\"Generated Prompt:\")\n",
        "print(prompt)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "response = simple_chatbot(prompt)\n",
        "print(\"ü§ñ Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4: Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple prompt:\n",
            "15% of 240 is 36.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Chain of thought prompt:\n",
            "1. We need to find 15% of 240.\n",
            "2. To convert 15% to decimal, we divide it by 100: 15% = 0.15\n",
            "3. Multiply 0.15 by 240: 0.15 * 240 = 36\n",
            "4. The final answer is 36. \n",
            "\n",
            "Therefore, 15% of 240 is 36.\n"
          ]
        }
      ],
      "source": [
        "# Without chain of thought\n",
        "simple_math = \"What is 15% of 240?\"\n",
        "\n",
        "# With chain of thought\n",
        "cot_math = \"\"\"\n",
        "What is 15% of 240?\n",
        "\n",
        "Let's solve this step by step:\n",
        "1. First, understand what we need to find\n",
        "2. Convert the percentage to decimal\n",
        "3. Multiply\n",
        "4. Give the final answer\n",
        "\"\"\"\n",
        "\n",
        "print(\"Simple prompt:\")\n",
        "print(simple_chatbot(simple_math))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"Chain of thought prompt:\")\n",
        "print(simple_chatbot(cot_math))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Try It Yourself!\n",
        "\n",
        "**Exercise:** Create a prompt template for code explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create a template that explains code in simple terms\n",
        "\n",
        "CODE_EXPLAIN_TEMPLATE = \"\"\"\n",
        "# TODO: Create your template here\n",
        "# Variables to include: {code}, {language}, {skill_level}\n",
        "\"\"\"\n",
        "\n",
        "# Test it with a code snippet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéØ Summary & Key Takeaways\n",
        "\n",
        "## What We Learned:\n",
        "\n",
        "### 1. **LLM Fundamentals**\n",
        "- Understanding tokens, temperature, and context windows\n",
        "- Making API calls to OpenAI\n",
        "- Examining response objects\n",
        "\n",
        "### 2. **Building Chatbots**\n",
        "- Single-turn vs. multi-turn conversations\n",
        "- System prompts for personality\n",
        "- Conversation history management\n",
        "\n",
        "### 3. **Memory Management**\n",
        "- Maintaining conversation context\n",
        "- Sliding window approach for token limits\n",
        "- Trade-offs between memory and performance\n",
        "\n",
        "### 4. **Streaming Responses**\n",
        "- Creating responsive, real-time chatbots\n",
        "- Handling streamed chunks\n",
        "\n",
        "### 5. **Prompt Engineering**\n",
        "- Writing clear, specific prompts\n",
        "- Few-shot learning with examples\n",
        "- Prompt templates for reusability\n",
        "- Chain of thought reasoning\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Next Steps:\n",
        "\n",
        "### Exercises for This Week:\n",
        "\n",
        "**Exercise 1 (Due Monday):** `02_exercise_personal_assistant.ipynb`\n",
        "- Build a personal assistant chatbot\n",
        "- Implement memory and personalities\n",
        "- Add prompt templates\n",
        "\n",
        "**Exercise 2 (Due Friday):** `03_exercise_domain_chatbot.ipynb`\n",
        "- Create a domain-specific chatbot\n",
        "- Implement streaming\n",
        "- Add input validation\n",
        "\n",
        "---\n",
        "\n",
        "## ü§î Reflection Questions:\n",
        "\n",
        "1. When would you use high vs. low temperature?\n",
        "2. Why is conversation history important?\n",
        "3. What are the trade-offs of keeping all conversation history?\n",
        "4. How does prompt engineering improve responses?\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Additional Resources:\n",
        "\n",
        "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
        "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
        "- [Best Practices for Prompt Engineering](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùì Questions?\n",
        "\n",
        "**Office Hours:** Monday & Friday check-ins  \n",
        "**Next Session:** Week 2 - LangChain Core Concepts\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Coding! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
